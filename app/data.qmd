---
title: "A Look at the Data"
---

# Approaches

Data are a crucial piece of this research project. As such, much of the effort and consideration went into selecting, crafting, and refactoring the datasets to suit the need as best as possible. Throughout this process, many things were tried and tossed away. These include the form of the data, and even methods for processing the data to make it easier for certain machine learning models to use them. In the end, an effective appraoch was discovered. Each of these snapshots will be covered below.

## The Old Data

Both the [wind](https://data.nrel.gov/submissions/54) and [solar](https://nsrdb.nrel.gov/) data used in this project orginally come from The National Renewable Energy Laboratory (NREL). The data comes with many attributes that are crucial to the analysis this project seeks to conduct, however, there are still pieces missing to truly acheive the full picture. Regardless, compared to other publically available datasets, these were the best suited for the job, and offered a good starting point.

### Structure and Make-up

The original datasets came with some basic attributes:

- longitude
- latitude
- wind speed
- solar irradiance
- capacity
- capacity factor

This, at first, is not enough data to begin predicting for energy generation and cost. For this, actual data on energy generation and cost is needed. With what was provided, these could be calculated. To start, a few simple calculations were used to find the energy and cost values desired. The attribute `capacity` denotes how much energy a certain technology can produce under ideal conditions. `Capacity factor` denotes what fraction of the technology's `capacity` is produced. With this, a simple estimate for energy generation can be calculated by multiplying both values together. For cost, many estimates exist that outline how much a technology costs per watt of installed capacity. Using these estimates, and multiplying by capacity, a value for cost can be aquired. This produced two datasets that took this shape:

#### Old Wind Data

{{< embed 1-datasets.ipynb#wind-old-dataset-preview >}}

#### Old Solar Data

{{< embed 1-datasets.ipynb#solar-old-dataset-preview >}}

The rough and simplistic nature of these estimates are generally okay as the concern of this project is to determine how well machine learning algorithms predict values, and not the correctness and integrity of the values themselves. Nonetheless, issues still appeared because of this. When training and testing the Random Forest Regressor, a feature importance graph was used to determine which attributes are the most important. This can also give insights into how the variables are used by other models as well. When running on the old wind data, the graph indicated one variable was dominating the dataset.

{{< embed 5-rf-old.ipynb#fig-feature-importances-old-wind >}}

{{< embed 5-rf-old.ipynb#feature-importances-old-wind >}}

Out of all of the features, `capacity` was shown to have the highest importance. This makes sense as both calculations are made up of direct proportions that scale based on `capacity`. This then indicates that the issue is not Random Forest exclusive, as this bias is present in the dataset, and not in how the algorithm interprets the dataset. Changes had to be implemeneted to remove this bias, which included adding more features and using more in-depth calculations to derive the features to be predicted. These are covered in [The New Data](#the-new-data).

### Clustering

The initial direction of this project was to look specifically into clustering techniques and cluster analysis. This process consists of grouping similar data into a specified number of clusters. When considering the geospatial data used for this project, clustering would be used to abstract coordinates in the form of longitude and latitude into a single value, a cluster ID. The cluster ID's would then represent the other features of the data point, allowing for predictions of other features to be made based off of a given cluster ID or coordinate location. The main two steps for performing the clustering is breifly outlined below:

1. Determine the number of optimal clusters for the dataset.

To do this, you just need to cluster the data with varying cluster numbers, score them, and graph the scores by the number of clusters and determine the point at which gains start to diminish.

{{< embed 6-data-clustering.ipynb#fig-optimal-clusters >}}

2. Cluster the data and plot the results

With the optimal number of clusters (or any desired number), the data can be clustered, with each data point given a cluster ID.

{{< embed 6-data-clustering.ipynb#clustered-data-preview >}}

Now that clusters are generated, they can be plotted and visualized.

{{< embed 6-data-clustering.ipynb#fig-clustered-data >}}

This not only shows the shape and dimensions of the data, but it also gives insights into how many clusters are suitable for the task. The scale of the data makes the number of optimal clusters here a bit ambiguous. Although around 20 seems to be the best, 20 clusters does not seem to represent the nuances of how geospatial data like wind speed and solar raditation changes throughout the country. This seems to indicate that this approach is a bit unreliable in that regard and could introduce more bias into the data and models.

Additionally, the clustering could also help increase the performance of the algorithms by lowering the number of input parameters. Simplifying the inputs allows the models to interpret the data easier, leading to better predictions. The issues described in [the structure and make-up section](#structure-and-make-up) further examplified this potential benefit. But for those same reasons, clustering ended up not being an ideal pathway forward. Not only would refactoring the data to remove the bias negate the benefits of clustering in the long run, but clustering also ends up become computationally expensive and tedious to implement and analyze given the nature of the data. For these reasons it was discarded as an aspect of the project.

## The New Data

To overcome the issues outlined above, the datasets were both refactored heavily to remove the bias present. This fixed structural issues, but also allowed for in-depth analysis of mahcine learning models without the time and efficiency overheads that would be present if clustering remained a crucial piece of the project. The process and results of refactoring both of the datasets are outlined below:

### Wind

To reduce the bias in the wind dataset, new features were added and a new equation was used that incorporated more features than just `capacity` to ensure a wider range of importance across the board.  Two new features were added:

1. State - the state that the latitude and longitude coordinates are within.
2. Levelized Cost of Energy (LCOE) - the average net cost of electricity generation for a generator over its lifetime.

Both of these features gave way for more inclusive calculations to be used for both energy generation and cost. The updated method to find the energy generation, $J$, for a given wind farm uses the following equation where $c$ is `capacity`, $\rho$ is the density of air, $A$ is the swept area of the wind turbine, $v$ is the `wind speed`, $h$ is the number of hours in a year, and $c_f$ is `capacity factor`:

> $J = \frac{c}{2}\left( \frac{1}{2}\rho Av^3\right)h c_f$
>
> Some values here are assumed constants: $\rho = 1.225 kg/m^3$, $A = 7854 m^2$, $h = 8760hrs$

Also, this equation was split at certain parts to increase the number of features, to aide the model in being able to track the relationships better. The splits added the features `available_wind_power` and `available_energy`.

Calculating the cost using LCOE is very simple. A technology's LCOE is measured in dollars per watt-hour of energy generated. The only other piece needed is the lifespan of the technology. For this project, a 20 year lifespan is assumed for all wind turbines. This results in this equation where $J$ is generated energy, $l$ is the LCOE of the technology in the state, and $t$ is the lifespan of the technology:

> $C = Jlt$

These equations incorporate crucial pieces of geospatial data that were not present in the previous calculation. It also adds other assumed constants, which add complexity to the calculation and obscure the relationships a bit, giving the model more agency to work out the connections. In the end, it results in a dataset that looks like this:

{{< embed 1-datasets.ipynb#wind-dataset-preview >}}

This dataset can be accessed and downloaded from [here](https://github.com/AidanNeeson/renewable-ml/blob/main/data/wind.csv).

### Solar

Similar to the wind dataset, bias was reduced by adding more features and using a new equation which incorporated more features. Like the wind data, to make these adjustments, `state` and `lcoe` needed to be added as features. On top of these, two other features were dervied as part of the calculation to determine the generated energy. These are as follows:

1. Array Area, where $c$ is `capacity`, $I$ is the `solar irradiance` and $c_f$ is the `capacity factor`.

> $A = \frac{24c}{Ic_f}$
>
> We multiply by 24 because $I$ is in units of $kWh/m^2/day$ and we want it in units of $kWh/m^2$ to ensure we end up with an area as the result.

2. Available Solar Resource, where $I$ is the `solar irradiance` and $A$ is the `array area`.

> $P = \frac{IA}{24*1000}$
>
> The 24 serves the same purpose as before, but the 1000 here changes the units from $kWh$ to $MWh$ to stay consistent all results.

With these, an equation for generated energy, $J$ can be derived, where $P$ is the `available solar resource`, $h$ is the number of hours in a year, $c_f$ is the `capacity factor`, and $q$ is the system losses, which is assumed to be 14%:

> $J = P h  c_f  q$

Same as the wind data, calculating the cost using the LCOE is very simple. A lifespan of 20 years is used again for consistency:

> $C = Jlt$

The chain of calculations incorporate more of the geospatial data, which exemplifies the purpose of the research project much more than what the old data had, even with clustering. Again, it allows the model to do its best at finding the relationship, compared to just recognizing a direct proportion like before. The resulting solar dataset looks like this:

{{< embed 1-datasets.ipynb#solar-dataset-preview >}}

This dataset is also available to be accessed and downloaded [here](https://github.com/AidanNeeson/renewable-ml/blob/main/data/solar.csv).