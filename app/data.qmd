---
title: "A Look at the Data"
notebook-view:
  - notebook: 1-datasets.ipynb
    title: "Data Showcase"
    url: https://colab.research.google.com/drive/1JHqernGZ5oXJ4FFCs0i1GkDQs56aWwRY?authuser=4#scrollTo=oWqeiw3pilBq
---

# Approaches

Data are a crucial piece of this research project. As such, much of the effort and consideration went into selecting, crafting, and refactoring the datasets to suit the need as best as possible. Throughout this process, many things were tried and tossed away. These include the form of the data, and even methods for processing the data to make it easier for certain machine learning models to use them. In the end, an effective appraoch was discovered. Each of these snapshots will be covered below.

## The Old Data

Both the [wind](https://data.nrel.gov/submissions/54) and [solar](https://nsrdb.nrel.gov/) data used in this project orginally come from The National Renewable Energy Laboratory (NREL). The data comes with many attributes that are crucial to the analysis this project seeks to conduct, however, there are still pieces missing to truly acheive the full picture. Regardless, compared to other publically available datasets, these were the best suited for the job, and offered a good starting point.

### Structure and Make-up

The original datasets came with some basic attributes:

- longitude
- latitude
- wind speed
- solar irradiance
- capacity
- capacity factor

This, at first, is not enough data to begin predicting for energy generation and cost. For this, actual data on energy generation and cost is needed. With what was provided, these could be calculated. To start, a few simple calculations were used to find the energy and cost values desired. The attribute `capacity` denotes how much energy a certain technology can produce under ideal conditions. `Capacity factor` denotes what fraction of the technology's `capacity` is produced. With this, a simple estimate for energy generation can be calculated by multiplying both values together. For cost, many estimates exist that outline how much a technology costs per watt of installed capacity. Using these estimates, and multiplying by capacity, a value for cost can be aquired. This produced two datasets that took this shape:

#### Old Wind Data

{{< embed 1-datasets.ipynb#wind-old-dataset-preview >}}

#### Old Solar Data

{{< embed 1-datasets.ipynb#solar-old-dataset-preview >}}

The rough and simplistic nature of these estimates are generally okay as the concern of this project is to determine how well machine learning algorithms predict values, and not the correctness and integrity of the values themselves. Nonetheless, issues still appeared because of this. When training and testing the Random Forest Regressor, a feature importance graph was used to determine which attributes are the most important. This can also give insights into how the variables are used by other models as well. When running on the old wind data, the graph indicated one variable was dominating the dataset.

{{< embed 5-rf-old.ipynb#feature-importance-graph-old-wind >}}

{{< embed 5-rf-old.ipynb#feature-importances-old-wind >}}

Out of all of the features, `capacity` was shown to have the highest importance. This makes sense as both calculations are made up of direct proportions that scale based on `capacity`. This then indicates that the issue is not Random Forest exclusive, as this bias is present in the dataset, and not in how the algorithm interprets the dataset. Changes had to be implemeneted to remove this bias, which included adding more features and using more in-depth calculations to derive the features to be predicted. These are covered in [The New Data](#the-new-data).

### Clustering

The initial direction of this project was to look specifically into clustering techniques and cluster analysis. This process consists of grouping similar data into a specified number of clusters. When considering the geospatial data used for this project, clustering would be used to abstract coordinates in the form of longitude and latitude into a single value, a cluster ID. The cluster ID's would then represent the other features of the data point, allowing for predictions of other features to be made based off of a given cluster ID or coordinate location. The main two steps for performing the clustering is breifly outlined below:

1. Determine the number of optimal clusters for the dataset.

To do this, you just need to cluster the data with varying cluster numbers, score them, and graph the scores by the number of clusters and determine the point at which gains start to diminish.

{{< embed 8-data-clustering.ipynb#fig-optimal-clusters >}}

2. Cluster the data and plot the results

With the optimal number of clusters (or any desired number), the data can be clustered, with each data point given a cluster ID.

{{< embed 8-data-clustering.ipynb#clustered-data-preview >}}

Now that clusters are generated, they can be plotted and visualized.

{{< embed 8-data-clustering.ipynb#fig-clustered-data >}}

This not only shows the shape and dimensions of the data, but it also gives insights into how many clusters are suitable for the task. The scale of the data makes the number of optimal clusters here a bit ambiguous. Although around 20 seems to be the best, 20 clusters does not seem to represent the nuances of how geospatial data like wind speed and solar raditation changes throughout the country. This seems to indicate that this approach is a bit unreliable in that regard and could introduce more bias into the data and models.

Additionally, the clustering could also help increase the performance of the algorithms by lowering the number of input parameters. Simplifying the inputs allows the models to interpret the data easier, leading to better predictions. The issues described in [the structure and make-up section](#structure-and-make-up) further examplified this potential benefit. But for those same reasons, clustering ended up not being an ideal pathway forward. Not only would refactoring the data to remove the bias negate the benefits of clustering in the long run, but clustering also ends up become computationally expensive and tedious to implement and analyze given the nature of the data. For these reasons it was discarded as an aspect of the project.

## The New Data

To overcome the issues outlined above, the datasets were both refactored heavily to remove the bias present. This fixed structural issues, but also allowed for in-depth analysis of mahcine learning models without the time and efficiency overheads that would be present if clustering remained a crucial piece of the project.

### Wind

### Solar
