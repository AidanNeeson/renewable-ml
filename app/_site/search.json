[
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "Support Vector Regression on Geospatial Data",
    "section": "",
    "text": "A Support Vector Machine Regression, or Support Vector Regression (SVR), is a machine learning algorithm that uses supervised learning to find a hyperplane that fits the data points in continuous space. What this means, is that the model tries to fit a curve that passes through as many data points as possible within a certain area, called a margin. This approach can lend itself to reducing prediction error and creating good representations of the data, even in non-linear formats. It has also has seen its usefullness in geospatial applications for both classification and regression purposes. Because of this, it has potential to be a good model for this application. However, the method an SVR takes can also lend itself to be unreliable in many scenarios depending on how the data represents itself, which will be seen later.\n\n\nFor a more detailed description of getting to a trained model, navigate to the Random Forest page. There, most of the important high level concepts are discussed. On this page, only the important pieces relating directly to an SVR will be covered for the sake of reducing unecessary repetition.\n\n\nWe will be using the wind data again for the SVR, and to reduce bias we start by shuffling it:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n70816\n70816\n42.770447\n-105.846069\nWyoming\nonshore\n9.52\n30\n12\n0.429\n24.903425\n218154.00560\n93588.068410\n5.615284e+07\n\n\n45705\n45705\n40.527233\n-86.395111\nIndiana\nonshore\n7.38\n37\n16\n0.445\n15.468780\n135506.51270\n60300.398130\n4.462229e+07\n\n\n60161\n60161\n41.261482\n-82.054718\nOhio\nonshore\n6.94\n39\n10\n0.369\n8.039803\n70428.67401\n25988.180710\n2.027078e+07\n\n\n66924\n66924\n42.571953\n-88.600952\nWisconsin\nonshore\n7.65\n33\n16\n0.420\n17.229445\n150929.93620\n63390.573220\n4.183778e+07\n\n\n87265\n87265\n43.272633\n-112.698792\nIdaho\nonshore\n5.89\n44\n6\n0.306\n2.948928\n25832.60690\n7904.777711\n6.956204e+06\n\n\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nAn SVR does to support multioutput, so each target must have a model that corresponds to it. Because of this, array generation and feature selection looks a bit different. The X array is the same as before, but things change with the y array. Instead of one, two need to be made, those being y_energy and y_cost, each for the two desired targets. All three of these arrays are then split into training and testing sets.\nX = df.loc[:, ['lat','long','capacity']]\ny_energy = df['generated_energy'].values\ny_cost = df['cost'].values\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_energy_train = y_energy[:100000]\ny_energy_test = y_energy[100000:]\ny_cost_train = y_cost[:100000]\ny_cost_test = y_cost[100000:]\nThe data is then scaled down using sklearn’s StandardScaler, making the data easier to be parsed and handled by the algorithm. Now the SVRs can be trained.\n\n\n\nKeeping things as simple as possible, the SVRs, coming from sklearn, are used out-of-the box. Some tuning was done to attempt to get better results, but nothing substantial was gained, so we will stick with basic models.\nenergy_reg = svm.SVR()\ncost_reg = svm.SVR()\nenergy_reg.fit(X_train, y_energy_train)\ncost_reg.fit(X_train, y_cost_train)\nNow predictions can be made using both models:\n\n\n\nPredictions\n----------------------\npredicted energy: 57328.67  actual energy: 40842.41 predicted cost: 39489716.81 actual cost: 36758169.87\npredicted energy: 49636.20  actual energy: 10659.56 predicted cost: 39480504.51 actual cost: 6822119.29\npredicted energy: 50298.13  actual energy: 9386.53  predicted cost: 39481146.84 actual cost: 8823336.22\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nThe predictions do not seem to vary much, and do not match up with the actual outputs either. From this point, a conclusion could already be drawn that the model is not performing up to standards, but to be sure, we have to analyze it more in-depth.\n\n\n\n\nApart from any model-specific evaluation techniques, the evaluation process remains the same as the other models. The models are graphed, metrics are gathered, and k-fold corss validation is used.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nThese plots even further indicate the poor performance of the SVR. The predictions fit nearly a straight line for both targets, no matter the input feature. This can most likely be attributed to the technique used by an SVR to make predictions in combination with the data. Although it is hard to see, the density of the datapoints near the bottom of the graph is a lot higher than in other parts (keep in mind there are about 120,000 data points). The SVR is trying to hit as many as possible within a certain area around the curve. Some features distribute these points in different ways, which can make it difficult for the SVR find the best fit. In the end it results in curves that do not look ideal, but to ensure this is the case, the metrics must be examined.\n\n\n\nThe metrics used are the same as the other two models: R2-score (R2), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n\n\n\nMetric  Score\n-----------------------\nenergy_r2   [0.123724]\ncost_r2 [0.00020249]\nenergy_rmse [29742.00648558]\ncost_rmse   [19613745.35965985]\nenergy_mape [1.08458669]\ncost_mape   [0.98070773]\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nThe metrics also indicate that the model does not perform well. The scale independent-metrics, R2 and MAPE, indicate very poor fits for both targets. RMSE is also too high to reasonably be explained away, especially when considering the averages for each target. The RMSE relative to target averages would indicate predictions are off by about 60% and above. Overall this is very poor performance, but this is only one representation of the data, which could be biased. The entire dataset must be used to ensure the analysis is as unbiased as possible.\n\n\n\nTo ensure the entire dataset it being used and tested, k-fold cross validation is used. Like with the other models, 10 folds will be used, and the metrics from above are recorded for evaluation purposes. The results for both energy and cost are below:\nEnergy:\n\n\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.0018450300789964902\nRMSE Average: 31730.72214388117\nMAPE Average: 1.1974198375900589\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n771.961335\n90.775717\n0.002921\n31909.960130\n1.150887\n\n\n1\n771.080194\n90.634640\n0.002406\n31278.957484\n1.194819\n\n\n2\n758.963423\n90.289549\n0.003153\n31580.604349\n1.276420\n\n\n3\n740.477523\n88.983850\n0.001673\n31700.498088\n1.158536\n\n\n4\n736.539896\n88.923703\n0.001296\n31842.196502\n1.261852\n\n\n5\n737.672136\n89.011240\n0.000597\n31694.965905\n1.201443\n\n\n6\n736.014961\n89.161244\n0.002357\n31440.657336\n1.128294\n\n\n7\n738.921449\n88.798775\n0.002150\n31583.352399\n1.273676\n\n\n8\n738.560741\n88.798225\n0.000881\n32331.064876\n1.176411\n\n\n9\n737.216899\n88.766471\n0.001017\n31944.964370\n1.151860\n\n\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nCost:\n\n\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: -0.00017107792075631288\nRMSE Average: 19730288.835201103\nMAPE Average: 0.9651219794341653\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n401.449423\n88.838005\n-0.000394\n1.963845e+07\n0.926044\n\n\n1\n401.821544\n89.089334\n-0.000016\n1.952591e+07\n0.965267\n\n\n2\n400.949044\n88.836812\n-0.000417\n1.976890e+07\n1.021886\n\n\n3\n401.101503\n88.819128\n-0.000002\n1.964222e+07\n0.946757\n\n\n4\n400.615725\n88.751960\n-0.000103\n1.992799e+07\n1.014057\n\n\n5\n401.312933\n89.032375\n-0.000370\n1.971245e+07\n0.962284\n\n\n6\n401.158882\n88.770521\n-0.000006\n1.930747e+07\n0.919042\n\n\n7\n400.537041\n88.876982\n0.000005\n1.968026e+07\n1.019527\n\n\n8\n400.704807\n88.816629\n-0.000183\n2.014325e+07\n0.954541\n\n\n9\n407.294723\n88.969792\n-0.000225\n1.995599e+07\n0.921815\n\n\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nThe results from k-fold cross validation reinforce the assumptions from before: the model does not perform well. In fact, it performs even worse overall than what could be gathered originally. It seems that although SVR can be a useful tool when modeling geospatial data, for this specific application, using it out-of-the-box is not effective. Fine tuning the model could give better results, but compared to the other models, SVR does not seem useful.\n\n\n\n\nThe solar data is not, and will, not be covered in detail like the wind data was above. It was concluded that the solar data is insufficient for the project’s goals. The metrics that come from a model trained on solar data indicate more than just poor results. It is omitted to reduce confusion and bring to light the more impactful results of this experiment.\n\n\n\nMetric  Score\n-----------------------\nenergy_r2   [-0.21012622]\ncost_r2 [-0.20850399]\nenergy_rmse [880927.68663408]\ncost_rmse   [6.87646055e+08]\nenergy_mape [81.95543371]\ncost_mape   [81.30935189]\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nNumbers that look like this are intrinsic of bad models or bad data. In this case, it is bad data. The issue stems from what data was available publically and feasible to work with. Many gaps needed to be filled in, and not enough data was available to fill these gaps in in a way that did not comprimise the dataset in the end. The notebook is still available to view for purposes of experiment replication and validation."
  },
  {
    "objectID": "svm.html#getting-a-trained-model",
    "href": "svm.html#getting-a-trained-model",
    "title": "Support Vector Regression on Geospatial Data",
    "section": "",
    "text": "For a more detailed description of getting to a trained model, navigate to the Random Forest page. There, most of the important high level concepts are discussed. On this page, only the important pieces relating directly to an SVR will be covered for the sake of reducing unecessary repetition.\n\n\nWe will be using the wind data again for the SVR, and to reduce bias we start by shuffling it:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n70816\n70816\n42.770447\n-105.846069\nWyoming\nonshore\n9.52\n30\n12\n0.429\n24.903425\n218154.00560\n93588.068410\n5.615284e+07\n\n\n45705\n45705\n40.527233\n-86.395111\nIndiana\nonshore\n7.38\n37\n16\n0.445\n15.468780\n135506.51270\n60300.398130\n4.462229e+07\n\n\n60161\n60161\n41.261482\n-82.054718\nOhio\nonshore\n6.94\n39\n10\n0.369\n8.039803\n70428.67401\n25988.180710\n2.027078e+07\n\n\n66924\n66924\n42.571953\n-88.600952\nWisconsin\nonshore\n7.65\n33\n16\n0.420\n17.229445\n150929.93620\n63390.573220\n4.183778e+07\n\n\n87265\n87265\n43.272633\n-112.698792\nIdaho\nonshore\n5.89\n44\n6\n0.306\n2.948928\n25832.60690\n7904.777711\n6.956204e+06\n\n\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nAn SVR does to support multioutput, so each target must have a model that corresponds to it. Because of this, array generation and feature selection looks a bit different. The X array is the same as before, but things change with the y array. Instead of one, two need to be made, those being y_energy and y_cost, each for the two desired targets. All three of these arrays are then split into training and testing sets.\nX = df.loc[:, ['lat','long','capacity']]\ny_energy = df['generated_energy'].values\ny_cost = df['cost'].values\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_energy_train = y_energy[:100000]\ny_energy_test = y_energy[100000:]\ny_cost_train = y_cost[:100000]\ny_cost_test = y_cost[100000:]\nThe data is then scaled down using sklearn’s StandardScaler, making the data easier to be parsed and handled by the algorithm. Now the SVRs can be trained.\n\n\n\nKeeping things as simple as possible, the SVRs, coming from sklearn, are used out-of-the box. Some tuning was done to attempt to get better results, but nothing substantial was gained, so we will stick with basic models.\nenergy_reg = svm.SVR()\ncost_reg = svm.SVR()\nenergy_reg.fit(X_train, y_energy_train)\ncost_reg.fit(X_train, y_cost_train)\nNow predictions can be made using both models:\n\n\n\nPredictions\n----------------------\npredicted energy: 57328.67  actual energy: 40842.41 predicted cost: 39489716.81 actual cost: 36758169.87\npredicted energy: 49636.20  actual energy: 10659.56 predicted cost: 39480504.51 actual cost: 6822119.29\npredicted energy: 50298.13  actual energy: 9386.53  predicted cost: 39481146.84 actual cost: 8823336.22\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nThe predictions do not seem to vary much, and do not match up with the actual outputs either. From this point, a conclusion could already be drawn that the model is not performing up to standards, but to be sure, we have to analyze it more in-depth."
  },
  {
    "objectID": "svm.html#analyzing-and-assessing-the-support-vector-regressor",
    "href": "svm.html#analyzing-and-assessing-the-support-vector-regressor",
    "title": "Support Vector Regression on Geospatial Data",
    "section": "",
    "text": "Apart from any model-specific evaluation techniques, the evaluation process remains the same as the other models. The models are graphed, metrics are gathered, and k-fold corss validation is used.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nThese plots even further indicate the poor performance of the SVR. The predictions fit nearly a straight line for both targets, no matter the input feature. This can most likely be attributed to the technique used by an SVR to make predictions in combination with the data. Although it is hard to see, the density of the datapoints near the bottom of the graph is a lot higher than in other parts (keep in mind there are about 120,000 data points). The SVR is trying to hit as many as possible within a certain area around the curve. Some features distribute these points in different ways, which can make it difficult for the SVR find the best fit. In the end it results in curves that do not look ideal, but to ensure this is the case, the metrics must be examined.\n\n\n\nThe metrics used are the same as the other two models: R2-score (R2), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n\n\n\nMetric  Score\n-----------------------\nenergy_r2   [0.123724]\ncost_r2 [0.00020249]\nenergy_rmse [29742.00648558]\ncost_rmse   [19613745.35965985]\nenergy_mape [1.08458669]\ncost_mape   [0.98070773]\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nThe metrics also indicate that the model does not perform well. The scale independent-metrics, R2 and MAPE, indicate very poor fits for both targets. RMSE is also too high to reasonably be explained away, especially when considering the averages for each target. The RMSE relative to target averages would indicate predictions are off by about 60% and above. Overall this is very poor performance, but this is only one representation of the data, which could be biased. The entire dataset must be used to ensure the analysis is as unbiased as possible.\n\n\n\nTo ensure the entire dataset it being used and tested, k-fold cross validation is used. Like with the other models, 10 folds will be used, and the metrics from above are recorded for evaluation purposes. The results for both energy and cost are below:\nEnergy:\n\n\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.0018450300789964902\nRMSE Average: 31730.72214388117\nMAPE Average: 1.1974198375900589\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n771.961335\n90.775717\n0.002921\n31909.960130\n1.150887\n\n\n1\n771.080194\n90.634640\n0.002406\n31278.957484\n1.194819\n\n\n2\n758.963423\n90.289549\n0.003153\n31580.604349\n1.276420\n\n\n3\n740.477523\n88.983850\n0.001673\n31700.498088\n1.158536\n\n\n4\n736.539896\n88.923703\n0.001296\n31842.196502\n1.261852\n\n\n5\n737.672136\n89.011240\n0.000597\n31694.965905\n1.201443\n\n\n6\n736.014961\n89.161244\n0.002357\n31440.657336\n1.128294\n\n\n7\n738.921449\n88.798775\n0.002150\n31583.352399\n1.273676\n\n\n8\n738.560741\n88.798225\n0.000881\n32331.064876\n1.176411\n\n\n9\n737.216899\n88.766471\n0.001017\n31944.964370\n1.151860\n\n\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nCost:\n\n\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: -0.00017107792075631288\nRMSE Average: 19730288.835201103\nMAPE Average: 0.9651219794341653\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n401.449423\n88.838005\n-0.000394\n1.963845e+07\n0.926044\n\n\n1\n401.821544\n89.089334\n-0.000016\n1.952591e+07\n0.965267\n\n\n2\n400.949044\n88.836812\n-0.000417\n1.976890e+07\n1.021886\n\n\n3\n401.101503\n88.819128\n-0.000002\n1.964222e+07\n0.946757\n\n\n4\n400.615725\n88.751960\n-0.000103\n1.992799e+07\n1.014057\n\n\n5\n401.312933\n89.032375\n-0.000370\n1.971245e+07\n0.962284\n\n\n6\n401.158882\n88.770521\n-0.000006\n1.930747e+07\n0.919042\n\n\n7\n400.537041\n88.876982\n0.000005\n1.968026e+07\n1.019527\n\n\n8\n400.704807\n88.816629\n-0.000183\n2.014325e+07\n0.954541\n\n\n9\n407.294723\n88.969792\n-0.000225\n1.995599e+07\n0.921815\n\n\n\n\n\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nThe results from k-fold cross validation reinforce the assumptions from before: the model does not perform well. In fact, it performs even worse overall than what could be gathered originally. It seems that although SVR can be a useful tool when modeling geospatial data, for this specific application, using it out-of-the-box is not effective. Fine tuning the model could give better results, but compared to the other models, SVR does not seem useful."
  },
  {
    "objectID": "svm.html#a-note-about-the-solar-data",
    "href": "svm.html#a-note-about-the-solar-data",
    "title": "Support Vector Regression on Geospatial Data",
    "section": "",
    "text": "The solar data is not, and will, not be covered in detail like the wind data was above. It was concluded that the solar data is insufficient for the project’s goals. The metrics that come from a model trained on solar data indicate more than just poor results. It is omitted to reduce confusion and bring to light the more impactful results of this experiment.\n\n\n\nMetric  Score\n-----------------------\nenergy_r2   [-0.21012622]\ncost_r2 [-0.20850399]\nenergy_rmse [880927.68663408]\ncost_rmse   [6.87646055e+08]\nenergy_mape [81.95543371]\ncost_mape   [81.30935189]\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nNumbers that look like this are intrinsic of bad models or bad data. In this case, it is bad data. The issue stems from what data was available publically and feasible to work with. Many gaps needed to be filled in, and not enough data was available to fill these gaps in in a way that did not comprimise the dataset in the end. The notebook is still available to view for purposes of experiment replication and validation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Renewable ML",
    "section": "",
    "text": "This website documents the computational , experimental, and analytical processes of training, testing, and evaluating three machine learning algorithms:\n\nRandom Forest\nSupport Vector Mahcine\nArtificial Neural Network\n\nEach of these algorithms were chosen because of their simplicity and presence in the acedemic literature. The goal of the project is to investigate if machine learning algorithms can produce accurate predictions of renewable energy array parameter, like generated energy and cost, when trained on locational data.\n\n\n\nThe Data page provides an in-depth walkthrough of the process of getting data for a machine learning project and ensuring it fits the necessary requirements. Each subsequent model page explains the workflow for each machine learning algorithm used and covers, in detail, the evaluation of each model.\n\n\n\nUltimately, the experiments yielded promising results, with the random forest having the capability to make accurate predictions a majority of the time. The other models, however, gave bad results and lacked the ability to make even remotely accurate predictions. In the end, this still contributes to the goal of the project by supplying evidence that machine learning models are able to interpret locational data effectively to arrive at valid predictions of array parameters.\nFor a detailed description of the project, methodologies, experiments, and results, view the thesis found here.\n\n\nThe world we all live in is a valuable thing, but we are not treating it with the respect it deserves. As industry expands and citizens become careless, global temperatures increase and sea levels rise. A dangerous future is ahead of us if we do not make a change. The most prominent and fastest growing solution to these problems we face is renewable energy, but barriers in society prevent the ease of development that would be desired to see. The ability to gain insights into the potential for renewables would serve as an effective way to promote the installation of low-carbon technologies. Many tools that exist in the world that attempt to fulfill this role, but they suffer due to being too narrowly scoped and complex. The need for quick and accurate predictions is dire. Machine learning has seated itself as a powerful tool in the computer science field for making predictions, but also in the energy sector for the purposes of forecasting. For these reasons it is seen as the perfect opportunity to apply its expertise. Using locational data like latitude and longitude, as well as a scaling quantity like capacity, three machine learning models are trained and their performance is evaluated. Using industry standard practices and metrics like k-fold cross-validation, R-squared, RMSE, and MAPE, the random forest was shown to be an accurate and effective predictor of renewable energy generation and cost for wind farms. Other models and data was proven to be insufficient due to these same metrics. Overall, machine learning has shown immense promise in applications like these."
  },
  {
    "objectID": "index.html#assessing-the-viability-and-accuracy-of-machine-learning-models-at-predicting-renewable-energy-array-parameters",
    "href": "index.html#assessing-the-viability-and-accuracy-of-machine-learning-models-at-predicting-renewable-energy-array-parameters",
    "title": "Renewable ML",
    "section": "",
    "text": "This website documents the computational , experimental, and analytical processes of training, testing, and evaluating three machine learning algorithms:\n\nRandom Forest\nSupport Vector Mahcine\nArtificial Neural Network\n\nEach of these algorithms were chosen because of their simplicity and presence in the acedemic literature. The goal of the project is to investigate if machine learning algorithms can produce accurate predictions of renewable energy array parameter, like generated energy and cost, when trained on locational data.\n\n\n\nThe Data page provides an in-depth walkthrough of the process of getting data for a machine learning project and ensuring it fits the necessary requirements. Each subsequent model page explains the workflow for each machine learning algorithm used and covers, in detail, the evaluation of each model.\n\n\n\nUltimately, the experiments yielded promising results, with the random forest having the capability to make accurate predictions a majority of the time. The other models, however, gave bad results and lacked the ability to make even remotely accurate predictions. In the end, this still contributes to the goal of the project by supplying evidence that machine learning models are able to interpret locational data effectively to arrive at valid predictions of array parameters.\nFor a detailed description of the project, methodologies, experiments, and results, view the thesis found here.\n\n\nThe world we all live in is a valuable thing, but we are not treating it with the respect it deserves. As industry expands and citizens become careless, global temperatures increase and sea levels rise. A dangerous future is ahead of us if we do not make a change. The most prominent and fastest growing solution to these problems we face is renewable energy, but barriers in society prevent the ease of development that would be desired to see. The ability to gain insights into the potential for renewables would serve as an effective way to promote the installation of low-carbon technologies. Many tools that exist in the world that attempt to fulfill this role, but they suffer due to being too narrowly scoped and complex. The need for quick and accurate predictions is dire. Machine learning has seated itself as a powerful tool in the computer science field for making predictions, but also in the energy sector for the purposes of forecasting. For these reasons it is seen as the perfect opportunity to apply its expertise. Using locational data like latitude and longitude, as well as a scaling quantity like capacity, three machine learning models are trained and their performance is evaluated. Using industry standard practices and metrics like k-fold cross-validation, R-squared, RMSE, and MAPE, the random forest was shown to be an accurate and effective predictor of renewable energy generation and cost for wind farms. Other models and data was proven to be insufficient due to these same metrics. Overall, machine learning has shown immense promise in applications like these."
  },
  {
    "objectID": "ann.html",
    "href": "ann.html",
    "title": "Artifical Neural Network Trained on Geospatial Data",
    "section": "",
    "text": "An Arificial Neural Network (ANN) is a machine learning model that attempts to make decisions in a way that is cimilar to the human brain. It does this by mimicking the way neurons work together to identify patterns and draw conclusions. Each network consists of layers, an inputs layer, hidden layers, and an output layer. The first and last are defined by the input features and the output targets. The hidden layers are dynamic and change depending on the scenario. Overall, it is a powerful model that has proved its effectiveness in geospatial applications. While they are typically more complex and require a lot of tuning to really begin to sense the power they have, it was still seen to be useful to explore the possibility of an ANN at its most basic form ebing useful in this application.\n\n\nFor a more detailed description of getting to a trained model, navigate to the Random Forest page. There, most of the important high level concepts are discussed. On this page, only the important pieces relating directly to an ANN will be covered for the sake of reducing unecessary repetition.\n\n\nThe data and preprocessing is the same as the Random Forest, since the ANN does support multioutput. Wind data will be used again. A preview of it after being shuffled is below, as well as the steps to select features and split into training and testing sets:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n79744\n79744\n43.146877\n-82.945618\nMichigan\nonshore\n7.18\n32\n16\n0.431\n14.244930\n124785.58400\n53782.58670\n34420855.49\n\n\n97161\n97161\n45.597183\n-104.014069\nSouth Dakota\nonshore\n7.69\n30\n16\n0.396\n17.501126\n153309.86550\n60710.70672\n36426424.03\n\n\n979\n979\n29.375134\n-90.818909\nLouisiana\nonshore\n6.15\n50\n16\n0.307\n8.951840\n78418.12075\n24074.36307\n24074363.07\n\n\n110376\n110376\n48.992310\n-101.517090\nNorth Dakota\nonshore\n7.44\n30\n16\n0.408\n15.849143\n138838.49260\n56646.10498\n33987662.99\n\n\n46495\n46495\n40.798595\n-88.867004\nIllinois\nonshore\n7.34\n36\n14\n0.405\n13.316289\n116650.69100\n47243.52987\n34015341.51\n\n\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nX = df.loc[:, ['lat','long','capacity']]\ny = df.loc[:, ['generated_energy','cost']]\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_train = y[:100000]\ny_test = y[100000:]\nNow the ANN can be trained.\n\n\n\nFor this project, the MLPRegressor from sklearn was used as the ANN. ANN’s typically require a lot of tuning to work effectively. Because of this, it was hard to use it in an out-of-the-box fashion like the other models. Some minimal changes needed to be made to ensure the ANN could function properly and converge most of the time. To achieve this, the solver used was lbfgs, as the other solvers would never converge. The number of epochs, defined by max_iter was set to 100 billion, as the ANN had issues converging when the number of epochs were lower. Hidden layer selection was done through a general rule of thumb when starting out using an ANN: the number of hidden layers should equal one, the number of neurons in the hidden layer should not be greater than twice the input layer, and the number of nuerons in each hidden layer, down to the output layer should decrease, ideally following some geometric sequence. Using this as a guide, hidden_layer_sizes was set to (4,). This specifies one hidden layer with four neurons.\nreg = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(4,), max_iter=100000000000)\nreg.fit(X_train, y_train)\nThis creates an ANN that looks like this:\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nThis satisfies each condition of the general rule of thumb, creating an ideal starting point for an ANN.\n\n\n\n\nApart from any model-specific evaluation techniques, the evaluation process remains the same as the other models. The models are graphed, metrics are gathered, and k-fold corss validation is used.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nThe curves are not very smooth, but they seem to represent the data pretty fair for most input features. The curve for capacity is less ideal that what one would like to see, as it does not fit clear upward trend as heavily as it should. Other than this, the curves fit generally well and indicate that the model may be doing an alright job at making predictions. More needs to be done to confirm this, however.\n\n\n\nA place to start would be a single report of metrics on one split of the data.\n\n\n\nMetric  Score\n-----------------------\nr2  [0.19333991 0.19953254]\nrmse    [   28635.94093632 17726324.27849805]\nmape    [0.80743714 0.62410014]\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nThese scores indicate that the model is performing poor. But, it also indicates that it does manage to represent some meaningful portion of the data well. Even though it leaves about 80% of the data unaccounted for, for a simple and under-tuned ANN, this performance is not relatively that bad. This is just one split of the data, it is best to ensure this is the case across the entire dataset.\n\n\n\nSimilar to the other models, 10 folds are used for this k-fold cross validation. The same metrics from above are also used to score the model with each split. The reported scores are below:\n\n\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.17550572322297842\nRMSE Average: 8955297.046558464\nMAPE Average: 0.7180028050220215\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n2.288576\n0.003992\n0.189615\n8.928638e+06\n0.654504\n\n\n1\n0.733644\n0.004988\n0.191056\n8.854147e+06\n0.730762\n\n\n2\n0.715144\n0.003999\n0.186500\n8.930511e+06\n0.652372\n\n\n3\n3.342753\n0.004004\n0.196500\n8.899258e+06\n0.715955\n\n\n4\n1.022440\n0.012998\n0.196300\n8.833933e+06\n0.677797\n\n\n5\n7.003229\n0.003998\n0.201263\n8.808617e+06\n0.666556\n\n\n6\n0.753272\n0.004013\n-0.000947\n9.860140e+06\n1.071770\n\n\n7\n0.807899\n0.004000\n0.200553\n8.879911e+06\n0.647242\n\n\n8\n0.673103\n0.003997\n0.204381\n8.752960e+06\n0.685099\n\n\n9\n1.356002\n0.002999\n0.189836\n8.804856e+06\n0.677972\n\n\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nThis shows that, on average, it performs slightly worse than initially thought. The difference is not that significant, so a similar conclusion can still be drawn. Overall, for an implementation of an ANN that was as simple as possible to obtain reliable results, these scores are nothing to scoff at, and can even go so far as to indicate that an ANN can be very promising if utilized to its maximum capacity in the future. Compared to the other models, it is not the best, but it also is not the worst. It has suited itself as a middle-of-the-road option until more research is done to unveil its treu strengths or weaknesses.\n\n\n\n\nThe solar data is not, and will, not be covered in detail like the wind data was above. It was concluded that the solar data is insufficient for the project’s goals. The metrics that come from a model trained on solar data indicate more than just poor results. It is omitted to reduce confusion and bring to light the more impactful results of this experiment.\n\n\n\nMetric  Score\n-----------------------\nenergy_r2   [-0.21012622]\ncost_r2 [-0.20850399]\nenergy_rmse [880927.68663408]\ncost_rmse   [6.87646055e+08]\nenergy_mape [81.95543371]\ncost_mape   [81.30935189]\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nNumbers that look like this are intrinsic of bad models or bad data. In this case, it is bad data. The issue stems from what data was available publically and feasible to work with. Many gaps needed to be filled in, and not enough data was available to fill these gaps in in a way that did not comprimise the dataset in the end. The notebook is still available to view for purposes of experiment replication and validation."
  },
  {
    "objectID": "ann.html#getting-a-trained-model",
    "href": "ann.html#getting-a-trained-model",
    "title": "Artifical Neural Network Trained on Geospatial Data",
    "section": "",
    "text": "For a more detailed description of getting to a trained model, navigate to the Random Forest page. There, most of the important high level concepts are discussed. On this page, only the important pieces relating directly to an ANN will be covered for the sake of reducing unecessary repetition.\n\n\nThe data and preprocessing is the same as the Random Forest, since the ANN does support multioutput. Wind data will be used again. A preview of it after being shuffled is below, as well as the steps to select features and split into training and testing sets:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n79744\n79744\n43.146877\n-82.945618\nMichigan\nonshore\n7.18\n32\n16\n0.431\n14.244930\n124785.58400\n53782.58670\n34420855.49\n\n\n97161\n97161\n45.597183\n-104.014069\nSouth Dakota\nonshore\n7.69\n30\n16\n0.396\n17.501126\n153309.86550\n60710.70672\n36426424.03\n\n\n979\n979\n29.375134\n-90.818909\nLouisiana\nonshore\n6.15\n50\n16\n0.307\n8.951840\n78418.12075\n24074.36307\n24074363.07\n\n\n110376\n110376\n48.992310\n-101.517090\nNorth Dakota\nonshore\n7.44\n30\n16\n0.408\n15.849143\n138838.49260\n56646.10498\n33987662.99\n\n\n46495\n46495\n40.798595\n-88.867004\nIllinois\nonshore\n7.34\n36\n14\n0.405\n13.316289\n116650.69100\n47243.52987\n34015341.51\n\n\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nX = df.loc[:, ['lat','long','capacity']]\ny = df.loc[:, ['generated_energy','cost']]\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_train = y[:100000]\ny_test = y[100000:]\nNow the ANN can be trained.\n\n\n\nFor this project, the MLPRegressor from sklearn was used as the ANN. ANN’s typically require a lot of tuning to work effectively. Because of this, it was hard to use it in an out-of-the-box fashion like the other models. Some minimal changes needed to be made to ensure the ANN could function properly and converge most of the time. To achieve this, the solver used was lbfgs, as the other solvers would never converge. The number of epochs, defined by max_iter was set to 100 billion, as the ANN had issues converging when the number of epochs were lower. Hidden layer selection was done through a general rule of thumb when starting out using an ANN: the number of hidden layers should equal one, the number of neurons in the hidden layer should not be greater than twice the input layer, and the number of nuerons in each hidden layer, down to the output layer should decrease, ideally following some geometric sequence. Using this as a guide, hidden_layer_sizes was set to (4,). This specifies one hidden layer with four neurons.\nreg = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(4,), max_iter=100000000000)\nreg.fit(X_train, y_train)\nThis creates an ANN that looks like this:\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nThis satisfies each condition of the general rule of thumb, creating an ideal starting point for an ANN."
  },
  {
    "objectID": "ann.html#analyzing-and-assessing-the-network",
    "href": "ann.html#analyzing-and-assessing-the-network",
    "title": "Artifical Neural Network Trained on Geospatial Data",
    "section": "",
    "text": "Apart from any model-specific evaluation techniques, the evaluation process remains the same as the other models. The models are graphed, metrics are gathered, and k-fold corss validation is used.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nThe curves are not very smooth, but they seem to represent the data pretty fair for most input features. The curve for capacity is less ideal that what one would like to see, as it does not fit clear upward trend as heavily as it should. Other than this, the curves fit generally well and indicate that the model may be doing an alright job at making predictions. More needs to be done to confirm this, however.\n\n\n\nA place to start would be a single report of metrics on one split of the data.\n\n\n\nMetric  Score\n-----------------------\nr2  [0.19333991 0.19953254]\nrmse    [   28635.94093632 17726324.27849805]\nmape    [0.80743714 0.62410014]\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nThese scores indicate that the model is performing poor. But, it also indicates that it does manage to represent some meaningful portion of the data well. Even though it leaves about 80% of the data unaccounted for, for a simple and under-tuned ANN, this performance is not relatively that bad. This is just one split of the data, it is best to ensure this is the case across the entire dataset.\n\n\n\nSimilar to the other models, 10 folds are used for this k-fold cross validation. The same metrics from above are also used to score the model with each split. The reported scores are below:\n\n\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.17550572322297842\nRMSE Average: 8955297.046558464\nMAPE Average: 0.7180028050220215\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n2.288576\n0.003992\n0.189615\n8.928638e+06\n0.654504\n\n\n1\n0.733644\n0.004988\n0.191056\n8.854147e+06\n0.730762\n\n\n2\n0.715144\n0.003999\n0.186500\n8.930511e+06\n0.652372\n\n\n3\n3.342753\n0.004004\n0.196500\n8.899258e+06\n0.715955\n\n\n4\n1.022440\n0.012998\n0.196300\n8.833933e+06\n0.677797\n\n\n5\n7.003229\n0.003998\n0.201263\n8.808617e+06\n0.666556\n\n\n6\n0.753272\n0.004013\n-0.000947\n9.860140e+06\n1.071770\n\n\n7\n0.807899\n0.004000\n0.200553\n8.879911e+06\n0.647242\n\n\n8\n0.673103\n0.003997\n0.204381\n8.752960e+06\n0.685099\n\n\n9\n1.356002\n0.002999\n0.189836\n8.804856e+06\n0.677972\n\n\n\n\n\n\n\nSource: Artificial Neural Network Process and Analysis for Wind Data\nThis shows that, on average, it performs slightly worse than initially thought. The difference is not that significant, so a similar conclusion can still be drawn. Overall, for an implementation of an ANN that was as simple as possible to obtain reliable results, these scores are nothing to scoff at, and can even go so far as to indicate that an ANN can be very promising if utilized to its maximum capacity in the future. Compared to the other models, it is not the best, but it also is not the worst. It has suited itself as a middle-of-the-road option until more research is done to unveil its treu strengths or weaknesses."
  },
  {
    "objectID": "ann.html#a-note-about-the-solar-data",
    "href": "ann.html#a-note-about-the-solar-data",
    "title": "Artifical Neural Network Trained on Geospatial Data",
    "section": "",
    "text": "The solar data is not, and will, not be covered in detail like the wind data was above. It was concluded that the solar data is insufficient for the project’s goals. The metrics that come from a model trained on solar data indicate more than just poor results. It is omitted to reduce confusion and bring to light the more impactful results of this experiment.\n\n\n\nMetric  Score\n-----------------------\nenergy_r2   [-0.21012622]\ncost_r2 [-0.20850399]\nenergy_rmse [880927.68663408]\ncost_rmse   [6.87646055e+08]\nenergy_mape [81.95543371]\ncost_mape   [81.30935189]\n\n\nSource: Support Vector Regression Process and Analysis for Wind Data\nNumbers that look like this are intrinsic of bad models or bad data. In this case, it is bad data. The issue stems from what data was available publically and feasible to work with. Many gaps needed to be filled in, and not enough data was available to fill these gaps in in a way that did not comprimise the dataset in the end. The notebook is still available to view for purposes of experiment replication and validation."
  },
  {
    "objectID": "5-rf-old.html",
    "href": "5-rf-old.html",
    "title": "Old Random Forest Model",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"../data/wind_old.csv\")\ndf = df.sample(frac=1)\n\nX = df.loc[:, [False, True, True, True, False, True, True, False, False]]\ny = df.loc[:, [False, False, False, False, False,False, False, True, True]]\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_train = y[:100000]\ny_test = y[100000:]\n\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nreg = RandomForestRegressor(random_state=0)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\n\n\n\n\n\nfeatures = ['lat','long','wind_speed','capacity','capacity_factor']\nimportances = reg.feature_importances_\nindices = np.argsort(importances)\n\nprint(\"Feature Importances\")\nprint('----------------------------')\nfor i in indices:\n    print(f\"{features[i]}: {importances[i]*100}\")\n\nFeature Importances\n----------------------------\nlat: 1.7415146179729832e-09\nlong: 4.556724073707158e-09\nwind_speed: 2.5874951940190983e-08\ncapacity_factor: 0.0005676720796435423\ncapacity: 99.99943229574717\n\n\n\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "5-rf-old.html#graphsmetrics",
    "href": "5-rf-old.html#graphsmetrics",
    "title": "Old Random Forest Model",
    "section": "",
    "text": "features = ['lat','long','wind_speed','capacity','capacity_factor']\nimportances = reg.feature_importances_\nindices = np.argsort(importances)\n\nprint(\"Feature Importances\")\nprint('----------------------------')\nfor i in indices:\n    print(f\"{features[i]}: {importances[i]*100}\")\n\nFeature Importances\n----------------------------\nlat: 1.7415146179729832e-09\nlong: 4.556724073707158e-09\nwind_speed: 2.5874951940190983e-08\ncapacity_factor: 0.0005676720796435423\ncapacity: 99.99943229574717\n\n\n\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "4-ann-solar.html",
    "href": "4-ann-solar.html",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor"
  },
  {
    "objectID": "4-ann-solar.html#imports",
    "href": "4-ann-solar.html#imports",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor"
  },
  {
    "objectID": "4-ann-solar.html#data-preprocessing",
    "href": "4-ann-solar.html#data-preprocessing",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nFirst, we read in the dataset.\n\ndf = pd.read_csv(\"../data/solar.csv\")\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nirradiance\nlcoe\ncapacity\ncapacity_factor\narray_area\navailable_solar_resource\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nlarge_community\n5.634079\n39\n5.00\n0.235\n90633.862770\n21.276596\n6132.00\n4782960.0\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nsmall_utility\n5.616413\n39\n5.00\n0.234\n91307.484990\n21.367521\n6132.00\n4782960.0\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nsmall_community\n5.746738\n39\n0.15\n0.239\n2621.097459\n0.627615\n183.96\n143488.8\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nsmall_utility\n5.742196\n39\n5.00\n0.239\n87439.036330\n20.920502\n6132.00\n4782960.0\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nsmall_utility\n5.817187\n39\n5.00\n0.242\n85241.850210\n20.661157\n6132.00\n4782960.0\n\n\n\n\n\n\n\nNow, we must shuffle the datasets to reduce bias.\n\ndf = df.sample(frac=1)\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nirradiance\nlcoe\ncapacity\ncapacity_factor\narray_area\navailable_solar_resource\ngenerated_energy\ncost\n\n\n\n\n3768\n3768\n34.595978\n-101.395447\nTexas\nsmall_residential\n6.161978\n39\n0.005\n0.257\n75.775358\n0.019455\n6.132\n4782.96\n\n\n7020\n7020\n35.248310\n-103.950714\nNew Mexico\nsmall_residential\n6.504286\n34\n0.005\n0.271\n68.078870\n0.018450\n6.132\n4169.76\n\n\n2886\n2886\n34.296539\n-101.171387\nTexas\nmedium_community\n6.143694\n39\n2.000\n0.256\n30519.098500\n7.812500\n2452.800\n1913184.00\n\n\n2459\n2459\n33.977848\n-103.510773\nNew Mexico\nsmall_utility\n6.502355\n34\n5.000\n0.271\n68099.084300\n18.450184\n6132.000\n4169760.00\n\n\n116\n116\n26.532219\n-98.917480\nTexas\nsmall_residential\n5.747884\n39\n0.005\n0.239\n87.352498\n0.020921\n6.132\n4782.96\n\n\n\n\n\n\n\nLooking at each dataset, we can identify which variables we want to use for our models.\n\nX = df.loc[:, ['lat','long','capacity']]\ny = df.loc[:, ['generated_energy','cost']]\n\nNow we split into training and testing sets, reserving about 80% for training and 20% for testing.\n\nX_train = X[:9500]\nX_test = X[9500:]\ny_train = y[:9500]\ny_test = y[9500:]\n\nModels typically perform better when input values are within a certain range, like [-1, 1] for example. We scale the data points appropriately.\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train\n\narray([[-0.13240247, -0.08048336, -0.44220054],\n       [ 0.03389717, -0.4094644 , -0.44220054],\n       [-0.20873875, -0.05163647, -0.43901059],\n       ...,\n       [-0.03585628, -0.56732468, -0.44196869],\n       [-0.03493828, -0.42838662, -0.44219254],\n       [ 1.38778536,  1.4910043 , -0.44219254]])"
  },
  {
    "objectID": "4-ann-solar.html#training-the-models",
    "href": "4-ann-solar.html#training-the-models",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "Training the Models",
    "text": "Training the Models\nNow that the data is pre-processed accordingly, the models can be trained and fit.\n\nreg = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(4,), max_iter=100000000000)\nreg.fit(X_train, y_train)\n\nMLPRegressor(hidden_layer_sizes=(4,), max_iter=100000000000, solver='lbfgs')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MLPRegressor?Documentation for MLPRegressoriFittedMLPRegressor(hidden_layer_sizes=(4,), max_iter=100000000000, solver='lbfgs') \n\n\nWith a trained model, predictions can now be made.\n\ndisplay = y_test.reset_index()\npreds = reg.predict(X_test)\nprint(\"Predictions\")\nprint(\"----------------------\")\nfor i in range(3):\n    print(f\"predicted energy: {preds[i][0]:.2f}\\tactual energy: {display.at[i, 'generated_energy']:.2f}\\tpredicted cost: {preds[i][1]:.2f}\\tactual cost: {display.at[i, 'cost']:.2f}\")\n\nPredictions\n----------------------\npredicted energy: 23.33 actual energy: 6132.00  predicted cost: 18878.16    actual cost: 4660320.00\npredicted energy: 23.33 actual energy: 18.40    predicted cost: 18878.16    actual cost: 14348.88\npredicted energy: 23.33 actual energy: 18.40    predicted cost: 18878.16    actual cost: 14348.88"
  },
  {
    "objectID": "4-ann-solar.html#testing-and-analyzing-the-models",
    "href": "4-ann-solar.html#testing-and-analyzing-the-models",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "Testing and Analyzing the Models",
    "text": "Testing and Analyzing the Models\nThis section contains metrics gathering and other figures that visualize the models and its results.\n\nMetrics\n\nScores and Error Values\nThe score being recored are the R2 score, Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n\nr2 = metrics.r2_score(y_test, preds, multioutput=\"raw_values\")\nrmse = metrics.root_mean_squared_error(y_test, preds, multioutput=\"raw_values\")\nmape = metrics.mean_absolute_percentage_error(y_test, preds, multioutput=\"raw_values\")\n\nprint(\"Metric\\tScore\")\nprint(\"-----------------------\")\nprint(f\"r2\\t{r2}\\nrmse\\t{rmse}\\nmape\\t{mape}\")\n\nMetric  Score\n-----------------------\nr2  [0.99902339 0.99667298]\nrmse    [2.38406355e+04 3.46787739e+07]\nmape    [0.84806197 0.8870492 ]\n\n\n\n\nK-Fold Cross Validation\nThis cross validation splits up the dataset into 10 unique folds, which are then used to test a model. The model is then scored using the same metrics outlined above: R2, RMSE, MAPE. This ensures the scoring is rigorous, and the entire dataset is used.\n\nkf = KFold(n_splits=10, shuffle=True)\nkf_cv_scores = cross_validate(reg, X, y, cv=kf, scoring={\"r2\":metrics.make_scorer(score_func=metrics.r2_score),\n \"rmse\":metrics.make_scorer(score_func=metrics.root_mean_squared_error),\n \"mape\":metrics.make_scorer(score_func=metrics.mean_absolute_percentage_error)})\nkf_cv_df = pd.DataFrame.from_dict(kf_cv_scores)\nmeans = kf_cv_df.mean()\nprint(\"10-Fold Cross Validation Scores\")\nprint(\"----------------------------------------------------\")\nprint(f\"R2 Average: {means.iloc[2]}\")\nprint(f\"RMSE Average: {means.iloc[3]}\")\nprint(f\"MAPE Average: {means.iloc[4]}\")\nkf_cv_df\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.7025267979882951\nRMSE Average: 60401706.12189176\nMAPE Average: 153.45564241338886\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n0.775316\n0.015012\n0.920262\n2.345819e+07\n294.079767\n\n\n1\n0.102844\n0.004999\n0.993852\n2.922157e+07\n190.852911\n\n\n2\n0.119983\n0.005017\n0.993431\n2.990562e+07\n209.905686\n\n\n3\n0.646986\n0.004013\n-0.466820\n3.201413e+07\n152.461293\n\n\n4\n0.068974\n0.005002\n0.993574\n3.179233e+07\n139.604871\n\n\n5\n0.340269\n0.006996\n0.995613\n2.879659e+07\n0.784094\n\n\n6\n1.826744\n0.008000\n0.808695\n2.644997e+07\n40.465054\n\n\n7\n0.458003\n0.004999\n0.991088\n2.961710e+07\n355.154285\n\n\n8\n0.013999\n0.004001\n-0.199207\n3.461493e+08\n1.006037\n\n\n9\n0.073999\n0.004001\n0.994779\n2.661224e+07\n150.242426\n\n\n\n\n\n\n\n\n\n\nGraphs\nGraphs of the Random Forest model fits on each of the input features, for each target.\n\nplot_lat_x = X[9500:].loc[:,['lat']].sort_values(by=['lat'])\nplot_long_x = X[9500:].loc[:,['long']].sort_values(by=['long'])\nplot_cap_x = X[9500:].loc[:,['capacity']].sort_values(by=['capacity'])\nplot_energy_y = pd.DataFrame(preds).loc[:,[0]].sort_values(by=[0])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Generated Energy(MWh)\")\naxis[0].set_title(\"Artificial Neural Network: Generated Energy vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Generated Energy(MWh)\")\naxis[1].set_title(\"Artificial Neural Network: Generated Energy vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Generated Energy(MWh)\")\naxis[2].set_title(\"Artificial Neural Network: Generated Energy vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nplot_cost_y = pd.DataFrame(preds).loc[:,[1]].sort_values(by=[1])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Cost($)\")\naxis[0].set_title(\"Artificial Neural Network: Cost vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Cost($)\")\naxis[1].set_title(\"Artificial Neural Network: Cost vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Cost($)\")\naxis[2].set_title(\"Artificial Neural Network: Cost vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nA Graph that models the structure of a nueral network as a visual aid.\n\n# Code originally comes from @craffel on GitHub: https://gist.github.com/craffel/2d727968c3aaebd10359\n# Updates were made to make it compatible with Python 3\ndef draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n    '''\n    Draw a neural network cartoon using matplotilb.\n    \n    :usage:\n        &gt;&gt;&gt; fig = plt.figure(figsize=(12, 12))\n        &gt;&gt;&gt; draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n    \n    :parameters:\n        - ax : matplotlib.axes.AxesSubplot\n            The axes on which to plot the cartoon (get e.g. by plt.gca())\n        - left : float\n            The center of the leftmost node(s) will be placed here\n        - right : float\n            The center of the rightmost node(s) will be placed here\n        - bottom : float\n            The center of the bottommost node(s) will be placed here\n        - top : float\n            The center of the topmost node(s) will be placed here\n        - layer_sizes : list of int\n            List of layer sizes, including input and output dimensionality\n    '''\n    n_layers = len(layer_sizes)\n    v_spacing = (top - bottom)/float(max(layer_sizes))\n    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n    # Nodes\n    for n, layer_size in enumerate(layer_sizes):\n        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n        for m in range(layer_size):\n            circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/4.,\n                                color='w', ec='k', zorder=4)\n            ax.add_artist(circle)\n    # Edges\n    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n        for m in range(layer_size_a):\n            for o in range(layer_size_b):\n                line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n                                  [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n                ax.add_artist(line)\n\nfig = plt.figure(figsize=(12, 12))\nax = fig.gca()\nax.axis('off')\ndraw_neural_net(ax, .1, .9, .1, .9, [3, 4, 2])\n\nplt.text(.06, .80, \"Input Layer\")\nplt.text(.45, .90, \"Hidden Layer\")\nplt.text(.85, .70, \"Output Layer\")\n\nplt.show()"
  },
  {
    "objectID": "3-svm-solar.html",
    "href": "3-svm-solar.html",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "3-svm-solar.html#imports",
    "href": "3-svm-solar.html#imports",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "3-svm-solar.html#data-preprocessing",
    "href": "3-svm-solar.html#data-preprocessing",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nFirst, we read in the dataset.\n\ndf = pd.read_csv(\"../data/solar.csv\")\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nirradiance\nlcoe\ncapacity\ncapacity_factor\narray_area\navailable_solar_resource\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nlarge_community\n5.634079\n39\n5.00\n0.235\n90633.862770\n21.276596\n6132.00\n4782960.0\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nsmall_utility\n5.616413\n39\n5.00\n0.234\n91307.484990\n21.367521\n6132.00\n4782960.0\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nsmall_community\n5.746738\n39\n0.15\n0.239\n2621.097459\n0.627615\n183.96\n143488.8\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nsmall_utility\n5.742196\n39\n5.00\n0.239\n87439.036330\n20.920502\n6132.00\n4782960.0\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nsmall_utility\n5.817187\n39\n5.00\n0.242\n85241.850210\n20.661157\n6132.00\n4782960.0\n\n\n\n\n\n\n\nNow, we must shuffle the datasets to reduce bias.\n\ndf = df.sample(frac=1)\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nirradiance\nlcoe\ncapacity\ncapacity_factor\narray_area\navailable_solar_resource\ngenerated_energy\ncost\n\n\n\n\n10549\n10549\n43.146587\n-93.410431\nIowa\nsmall_residential\n4.739488\n45\n0.005\n0.197\n128.523807\n0.025381\n6.132\n5518.80\n\n\n4229\n4229\n32.752232\n-116.028137\nCalifornia\nsmall_utility\n6.851059\n41\n5.000\n0.285\n61458.036490\n17.543860\n6132.000\n5028240.00\n\n\n4414\n4414\n35.003220\n-101.967285\nTexas\nlarge_community\n6.262835\n39\n5.000\n0.261\n73412.458260\n19.157088\n6132.000\n4782960.00\n\n\n10478\n10478\n42.722588\n-101.106384\nNebraska\nlarge_community\n5.437910\n43\n5.000\n0.227\n97212.780910\n22.026432\n6132.000\n5273520.00\n\n\n8767\n8767\n35.880466\n-99.368011\nOklahoma\nmedium_residential\n5.918760\n41\n0.010\n0.247\n164.166121\n0.040486\n12.264\n10056.48\n\n\n\n\n\n\n\nLooking at each dataset, we can identify which variables we want to use for our models.\n\nX = df.loc[:, ['lat','long','capacity']]\ny_energy = df['generated_energy'].values\ny_cost = df['cost'].values\n\nNow we split into training and testing sets, reserving about 80% for training and 20% for testing.\n\nX_train = X[:9500]\nX_test = X[9500:]\ny_energy_train = y_energy[:9500]\ny_energy_test = y_energy[9500:]\ny_cost_train = y_cost[:9500]\ny_cost_test = y_cost[9500:]\n\nModels typically perform better when input values are within a certain range, like [-1, 1] for example. We scale the data points appropriately.\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train\n\narray([[ 2.03229033,  0.92873139, -0.43905719],\n       [-0.5962383 , -1.96682005, -0.43098755],\n       [-0.02700756, -0.16672944, -0.43098755],\n       ...,\n       [-0.12947216, -0.24196866, -0.43905719],\n       [ 2.41359832,  0.38433555, -0.43904104],\n       [-0.0831422 ,  0.65286873, -0.43098755]])"
  },
  {
    "objectID": "3-svm-solar.html#training-the-models",
    "href": "3-svm-solar.html#training-the-models",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "Training the Models",
    "text": "Training the Models\nNow that the data is pre-processed accordingly, the models can be trained and fit.\n\nenergy_reg = svm.SVR()\ncost_reg = svm.SVR()\nenergy_reg.fit(X_train, y_energy_train)\ncost_reg.fit(X_train, y_cost_train)\n\nSVR()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVR?Documentation for SVRiFittedSVR() \n\n\nWith a trained model, predictions can now be made.\n\nenergy_display = y_energy_test\ncost_display = y_cost_test\nenergy_preds = energy_reg.predict(X_test)\ncost_preds = cost_reg.predict(X_test)\nprint(\"Predictions\")\nprint(\"----------------------\")\nfor i in range(3):\n    print(f\"predicted energy: {energy_preds[i]:.2f}\\tactual energy: {energy_display[i]:.2f}\\tpredicted cost: {cost_preds[i]:.2f}\\tactual cost: {cost_display[i]:.2f}\")\n\nPredictions\n----------------------\npredicted energy: 2494.27   actual energy: 6.13 predicted cost: 1913464.63  actual cost: 4905.60\npredicted energy: 2438.26   actual energy: 2452.80  predicted cost: 1913207.65  actual cost: 1913184.00\npredicted energy: 2753.60   actual energy: 183.96   predicted cost: 1913890.42  actual cost: 172922.40"
  },
  {
    "objectID": "3-svm-solar.html#testing-and-analyzing-the-models",
    "href": "3-svm-solar.html#testing-and-analyzing-the-models",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "Testing and Analyzing the Models",
    "text": "Testing and Analyzing the Models\nThis section contains metrics gathering and other figures that visualize the models and its results.\n\nMetrics\n\nScores and Error Values\nThe score being recored are the R2 score, Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n\nenergy_r2 = metrics.r2_score(y_energy_test, energy_preds, multioutput=\"raw_values\")\nenergy_rmse = metrics.root_mean_squared_error(y_energy_test, energy_preds, multioutput=\"raw_values\")\nenergy_mape = metrics.mean_absolute_percentage_error(y_energy_test, energy_preds, multioutput=\"raw_values\")\n\ncost_r2 = metrics.r2_score(y_cost_test, cost_preds, multioutput=\"raw_values\")\ncost_rmse = metrics.root_mean_squared_error(y_cost_test, cost_preds, multioutput=\"raw_values\")\ncost_mape = metrics.mean_absolute_percentage_error(y_cost_test, cost_preds, multioutput=\"raw_values\")\n\nprint(\"Metric\\tScore\")\nprint(\"-----------------------\")\nprint(f\"energy_r2\\t{energy_r2}\\ncost_r2\\t{cost_r2}\\nenergy_rmse\\t{energy_rmse}\\ncost_rmse\\t{cost_rmse}\\nenergy_mape\\t{energy_mape}\\ncost_mape\\t{cost_mape}\")\n\nMetric  Score\n-----------------------\nenergy_r2   [-0.21012622]\ncost_r2 [-0.20850399]\nenergy_rmse [880927.68663408]\ncost_rmse   [6.87646055e+08]\nenergy_mape [81.95543371]\ncost_mape   [81.30935189]\n\n\n\n\nK-Fold Cross Validation\nThis cross validation splits up the dataset into 10 unique folds, which are then used to test a model. The model is then scored using the same metrics outlined above: R2, RMSE, MAPE. This ensures the scoring is rigorous, and the entire dataset is used.\n\nkf = KFold(n_splits=10, random_state=0, shuffle=True)\nkf_cv_scores = cross_validate(energy_reg, X, y_energy, cv=kf, scoring={\"r2\":metrics.make_scorer(score_func=metrics.r2_score),\n \"rmse\":metrics.make_scorer(score_func=metrics.root_mean_squared_error),\n \"mape\":metrics.make_scorer(score_func=metrics.mean_absolute_percentage_error)})\nkf_cv_df = pd.DataFrame.from_dict(kf_cv_scores)\nmeans = kf_cv_df.mean()\nprint(\"10-Fold Cross Validation Scores\")\nprint(\"----------------------------------------------------\")\nprint(f\"R2 Average: {means.iloc[2]}\")\nprint(f\"RMSE Average: {means.iloc[3]}\")\nprint(f\"MAPE Average: {means.iloc[4]}\")\nkf_cv_df\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: -0.19080402178052483\nRMSE Average: 835719.6572480934\nMAPE Average: 83.11834411696152\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n5.711730\n1.212786\n-0.209988\n858270.866365\n77.702522\n\n\n1\n3.275041\n1.064659\n-0.175722\n825723.536395\n84.581029\n\n\n2\n3.531009\n1.017413\n-0.196909\n829640.908412\n78.586396\n\n\n3\n3.094635\n0.965153\n-0.190046\n798942.604556\n84.428920\n\n\n4\n3.407629\n1.058627\n-0.200488\n874252.589933\n87.739410\n\n\n5\n3.160279\n0.958006\n-0.192127\n847512.024413\n81.573179\n\n\n6\n3.356327\n0.942312\n-0.196893\n867838.629564\n80.135358\n\n\n7\n3.356798\n0.907112\n-0.171115\n804687.181740\n87.416628\n\n\n8\n3.331655\n0.908973\n-0.201225\n850576.101493\n82.517218\n\n\n9\n3.200842\n0.976853\n-0.173528\n799752.129609\n86.502780\n\n\n\n\n\n\n\n\nkf = KFold(n_splits=10, random_state=0, shuffle=True)\nkf_cv_scores = cross_validate(cost_reg, X, y_cost, cv=kf, scoring={\"r2\":metrics.make_scorer(score_func=metrics.r2_score),\n \"rmse\":metrics.make_scorer(score_func=metrics.root_mean_squared_error),\n \"mape\":metrics.make_scorer(score_func=metrics.mean_absolute_percentage_error)})\nkf_cv_df = pd.DataFrame.from_dict(kf_cv_scores)\nmeans = kf_cv_df.mean()\nprint(\"10-Fold Cross Validation Scores\")\nprint(\"----------------------------------------------------\")\nprint(f\"R2 Average: {means.iloc[2]}\")\nprint(f\"RMSE Average: {means.iloc[3]}\")\nprint(f\"MAPE Average: {means.iloc[4]}\")\nkf_cv_df\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: -0.19100059475135955\nRMSE Average: 657874231.9115219\nMAPE Average: 83.9877821694812\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n3.926678\n0.935226\n-0.208953\n6.800997e+08\n78.355280\n\n\n1\n3.605959\n0.954635\n-0.176153\n6.491585e+08\n86.265737\n\n\n2\n3.739908\n1.006299\n-0.195172\n6.610066e+08\n78.606084\n\n\n3\n3.506410\n0.996119\n-0.190897\n6.284168e+08\n84.443426\n\n\n4\n3.706270\n0.962898\n-0.200360\n6.923436e+08\n88.103419\n\n\n5\n3.648341\n0.965594\n-0.192996\n6.650218e+08\n82.503671\n\n\n6\n3.819171\n0.977612\n-0.196144\n6.854944e+08\n81.365376\n\n\n7\n3.566598\n0.932271\n-0.172927\n6.276688e+08\n88.890206\n\n\n8\n3.490718\n0.985235\n-0.200951\n6.666649e+08\n83.557596\n\n\n9\n3.594051\n0.949818\n-0.175453\n6.228673e+08\n87.787026\n\n\n\n\n\n\n\n\n\n\nGraphs\nGraphs of the SVR model fits on each on the input features, for each target.\n\nplot_lat_x = X[9500:].loc[:,['lat']].sort_values(by=['lat'])\nplot_long_x = X[9500:].loc[:,['long']].sort_values(by=['long'])\nplot_cap_x = X[9500:].loc[:,['capacity']].sort_values(by=['capacity'])\nplot_energy_y = pd.DataFrame(energy_preds).loc[:,[0]].sort_values(by=[0])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y_energy, color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Generated Energy(MWh)\")\naxis[0].set_title(\"Support Vector Regression: Generated Energy vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y_energy, color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Generated Energy(MWh)\")\naxis[1].set_title(\"Support Vector Regression: Generated Energy vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y_energy, color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Generated Energy(MWh)\")\naxis[2].set_title(\"Support Vector Regression: Generated Energy vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nplot_cost_y = pd.DataFrame(cost_preds).loc[:,[0]].sort_values(by=[0])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y_cost, color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Cost($)\")\naxis[0].set_title(\"Support Vector Regression: Cost vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y_cost, color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Cost($)\")\naxis[1].set_title(\"Support Vector Regression: Cost vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y_cost, color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Cost($)\")\naxis[2].set_title(\"Support Vector Regression: Cost vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "2-rf-solar.html",
    "href": "2-rf-solar.html",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree"
  },
  {
    "objectID": "2-rf-solar.html#imports",
    "href": "2-rf-solar.html#imports",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree"
  },
  {
    "objectID": "2-rf-solar.html#data-preprocessing",
    "href": "2-rf-solar.html#data-preprocessing",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nFirst, we read in the dataset.\n\ndf = pd.read_csv(\"../data/solar.csv\")\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nirradiance\nlcoe\ncapacity\ncapacity_factor\narray_area\navailable_solar_resource\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nlarge_community\n5.634079\n39\n5.00\n0.235\n90633.862770\n21.276596\n6132.00\n4782960.0\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nsmall_utility\n5.616413\n39\n5.00\n0.234\n91307.484990\n21.367521\n6132.00\n4782960.0\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nsmall_community\n5.746738\n39\n0.15\n0.239\n2621.097459\n0.627615\n183.96\n143488.8\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nsmall_utility\n5.742196\n39\n5.00\n0.239\n87439.036330\n20.920502\n6132.00\n4782960.0\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nsmall_utility\n5.817187\n39\n5.00\n0.242\n85241.850210\n20.661157\n6132.00\n4782960.0\n\n\n\n\n\n\n\nNow, we must shuffle the datasets to reduce bias.\n\ndf = df.sample(frac=1)\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nirradiance\nlcoe\ncapacity\ncapacity_factor\narray_area\navailable_solar_resource\ngenerated_energy\ncost\n\n\n\n\n6903\n6903\n35.340389\n-102.449249\nTexas\nmedium_utility\n6.323517\n39\n500.000\n0.263\n7.215506e+06\n1901.140684\n613200.000\n4.782960e+08\n\n\n5455\n5455\n34.969135\n-104.838562\nNew Mexico\nlarge_community\n6.552958\n34\n5.000\n0.273\n6.707818e+04\n18.315018\n6132.000\n4.169760e+06\n\n\n5800\n5800\n35.222565\n-101.987335\nTexas\nsmall_residential\n6.259271\n39\n0.005\n0.261\n7.345426e+01\n0.019157\n6.132\n4.782960e+03\n\n\n2889\n2889\n34.300049\n-101.105133\nTexas\nmedium_community\n6.184820\n39\n2.000\n0.258\n3.008115e+04\n7.751938\n2452.800\n1.913184e+06\n\n\n9752\n9752\n37.012718\n-80.288757\nVirginia\nsmall_community\n5.146555\n46\n0.150\n0.214\n3.268677e+03\n0.700935\n183.960\n1.692432e+05\n\n\n\n\n\n\n\nLooking at each dataset, we can identify which variables we want to use for our models.\n\nX = df.loc[:, ['lat','long','capacity']]\ny = df.loc[:, ['generated_energy','cost']]\n\nNow we split into training and testing sets, reserving about 80% for training and 20% for testing.\n\nX_train = X[:9500]\nX_test = X[9500:]\ny_train = y[:9500]\ny_test = y[9500:]\n\nModels typically perform better when input values are within a certain range, like [-1, 1] for example. We scale the data points appropriately.\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train\n\narray([[ 0.05942354, -0.22478554,  0.35692648],\n       [-0.03505453, -0.53086542, -0.43588483],\n       [ 0.02943926, -0.16561264, -0.44388501],\n       ...,\n       [-1.65987101,  0.4749176 ,  0.35692648],\n       [-0.40675385, -0.96669284, -0.43588483],\n       [ 0.11236886,  0.3340536 , -0.44388501]])"
  },
  {
    "objectID": "2-rf-solar.html#training-the-models",
    "href": "2-rf-solar.html#training-the-models",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "Training the Models",
    "text": "Training the Models\nNow that the data is pre-processed accordingly, the models can be trained and fit.\n\nreg = RandomForestRegressor()\nreg.fit(X_train, y_train)\n\nRandomForestRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor() \n\n\nWith a trained model, predictions can now be made.\n\ndisplay = y_test.reset_index()\npreds = reg.predict(X_test)\nprint(\"Predictions\")\nprint(\"----------------------\")\nfor i in range(3):\n    print(f\"predicted energy: {preds[i][0]:.2f}\\tactual energy: {display.at[i, 'generated_energy']:.2f}\\tpredicted cost: {preds[i][1]:.2f}\\tactual cost: {display.at[i, 'cost']:.2f}\")\n\nPredictions\n----------------------\npredicted energy: 6132.00   actual energy: 6132.00  predicted cost: 5028240.00  actual cost: 5028240.00\npredicted energy: 613200.00 actual energy: 613200.00    predicted cost: 416976000.00    actual cost: 416976000.00\npredicted energy: 6132.00   actual energy: 6132.00  predicted cost: 5028240.00  actual cost: 5028240.00"
  },
  {
    "objectID": "2-rf-solar.html#testing-and-analyzing-the-models",
    "href": "2-rf-solar.html#testing-and-analyzing-the-models",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "Testing and Analyzing the Models",
    "text": "Testing and Analyzing the Models\nThis section contains metrics gathering and other figures that visualize the models and its results.\n\nMetrics\n\nScores and Error Values\nThe score being recored are the R2 score, Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n\nr2 = metrics.r2_score(y_test, preds, multioutput=\"raw_values\")\nrmse = metrics.root_mean_squared_error(y_test, preds, multioutput=\"raw_values\")\nmape = metrics.mean_absolute_percentage_error(y_test, preds, multioutput=\"raw_values\")\n\nprint(\"Metric\\tScore\")\nprint(\"-----------------------\")\nprint(f\"r2\\t{r2}\\nrmse\\t{rmse}\\nmape\\t{mape}\")\n\nMetric  Score\n-----------------------\nr2  [1.         0.99947801]\nrmse    [1.89379452e-12 1.36960035e+07]\nmape    [9.69285180e-16 4.32843758e-03]\n\n\n\n\nFeature Importances\nFeature importances give insights into the features that each decision tree in the random forest use to split most often. Results are portrayed in percentages.\n\nfeatures = ['lat','long','capacity',]\n\nimportances = reg.feature_importances_\nindices = np.argsort(importances)\n\nprint(\"Importances\")\nprint('----------------------')\nfor i in indices:\n    print(f\"{features[i]}: {importances[i]*100}\")\n\nImportances\n----------------------\nlong: 0.49924958098045885\nlat: 0.7828307672340137\ncapacity: 98.71791965178554\n\n\n\n\nK-Fold Cross Validation\nThis cross validation splits up the dataset into 10 unique folds, which are then used to test a model. The model is then scored using the same metrics outlined above: R2, RMSE, MAPE. This ensures the scoring is rigorous, and the entire dataset is used.\n\nkf = KFold(n_splits=10, random_state=0, shuffle=True)\nkf_cv_scores = cross_validate(reg, X, y, cv=kf, scoring={\"r2\":metrics.make_scorer(score_func=metrics.r2_score),\n \"rmse\":metrics.make_scorer(score_func=metrics.root_mean_squared_error),\n \"mape\":metrics.make_scorer(score_func=metrics.mean_absolute_percentage_error)})\nkf_cv_df = pd.DataFrame.from_dict(kf_cv_scores)\nmeans = kf_cv_df.mean()\nprint(\"10-Fold Cross Validation Scores\")\nprint(\"----------------------------------------------------\")\nprint(f\"R2 Average: {means.iloc[2]}\")\nprint(f\"RMSE Average: {means.iloc[3]}\")\nprint(f\"MAPE Average: {means.iloc[4]}\")\nkf_cv_df\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.999750656025779\nRMSE Average: 6617044.390212841\nMAPE Average: 0.0019449076617190596\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n1.632876\n0.015011\n0.999873\n4.897471e+06\n0.002525\n\n\n1\n1.631738\n0.014999\n0.999823\n5.617429e+06\n0.002217\n\n\n2\n1.622672\n0.015010\n0.999570\n8.502769e+06\n0.001829\n\n\n3\n1.667683\n0.017003\n0.999713\n7.136015e+06\n0.002001\n\n\n4\n1.637736\n0.015005\n0.999726\n7.230096e+06\n0.001571\n\n\n5\n1.633425\n0.015007\n0.999664\n7.606358e+06\n0.001857\n\n\n6\n1.658941\n0.016011\n0.999776\n6.570015e+06\n0.001679\n\n\n7\n1.636302\n0.015014\n0.999762\n6.738053e+06\n0.001821\n\n\n8\n1.638819\n0.015063\n0.999753\n6.439429e+06\n0.001767\n\n\n9\n1.669811\n0.015005\n0.999846\n5.432810e+06\n0.002182\n\n\n\n\n\n\n\n\n\n\nGraphs\nGraphs of the Random Forest model fits on each of the input features, for each target.\n\nplot_lat_x = X[9500:].loc[:,['lat']].sort_values(by=['lat'])\nplot_long_x = X[9500:].loc[:,['long']].sort_values(by=['long'])\nplot_cap_x = X[9500:].loc[:,['capacity']].sort_values(by=['capacity'])\nplot_energy_y = pd.DataFrame(preds).loc[:,[0]].sort_values(by=[0])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Generated Energy(MWh)\")\naxis[0].set_title(\"Random Forest Regression: Generated Energy vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Generated Energy(MWh)\")\naxis[1].set_title(\"Random Forest Regression: Generated Energy vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Generated Energy(MWh)\")\naxis[2].set_title(\"Random Forest Regression: Generated Energy vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nplot_cost_y = pd.DataFrame(preds).loc[:,[1]].sort_values(by=[1])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Cost($)\")\naxis[0].set_title(\"Random Forest Regression: Cost vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Cost($)\")\naxis[1].set_title(\"Random Forest Regression: Cost vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Cost($)\")\naxis[2].set_title(\"Random Forest Regression: Cost vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nA graph of the feature importances. This helps to visualize the magnitude of importance of each feature, and compare their impact against one another.\n\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nA graph of one of the decision trees in the random forest. This displays the decision making process the model takes to arive at predictions.\n\nfn = ['lat','long','capacity']\ncn = ['generated_energy','cost']\nplt.subplots(nrows=1, ncols=1, figsize=(4,4), dpi=800)\ntree.plot_tree(reg.estimators_[0],feature_names=fn,class_names=cn,filled=True, max_depth=3)\nplt.show()\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "1-datasets.html",
    "href": "1-datasets.html",
    "title": "Data Preview",
    "section": "",
    "text": "import pandas as pd\n\n\nwind_old_df = pd.read_csv('../data/wind_old.csv')\nsolar_old_df = pd.read_csv('../data/solar_old.csv')\nwind_df = pd.read_csv('../data/wind.csv')\nsolar_df = pd.read_csv('../data/solar.csv')\n\n\nwind_old_df.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nwind_speed\nfarm_type\ncapacity\ncapacity_factor\npower_generation\nestimated_cost\n\n\n\n\n0\n0\n23.510410\n-117.147260\n6.07\noffshore\n16\n0.169\n23687.04\n20800000\n\n\n1\n1\n24.007446\n-93.946777\n7.43\noffshore\n16\n0.302\n42328.32\n20800000\n\n\n2\n2\n25.069138\n-97.482483\n8.19\noffshore\n16\n0.375\n52560.00\n20800000\n\n\n3\n3\n25.069443\n-97.463135\n8.19\noffshore\n16\n0.375\n52560.00\n20800000\n\n\n4\n4\n25.069763\n-97.443756\n8.19\noffshore\n16\n0.376\n52700.16\n20800000\n\n\n\n\n\n\n\n\nsolar_old_df.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nirradiance\nfarm_type\ncapacity\ncapacity_factor\npower_generation\nestimated_cost\n\n\n\n\n0\n0\n25.896492\n-97.460358\n5.634079\nlarge_community\n5.00\n0.235\n10282.193270\n13300000\n\n\n1\n1\n26.032654\n-97.738098\n5.616413\nsmall_utility\n5.00\n0.234\n10249.953070\n13300000\n\n\n2\n2\n26.059063\n-97.208252\n5.746738\nsmall_community\n0.15\n0.239\n314.633929\n399000\n\n\n3\n3\n26.078449\n-98.073364\n5.742196\nsmall_utility\n5.00\n0.239\n10479.506980\n13300000\n\n\n4\n4\n26.143227\n-98.311340\n5.817187\nsmall_utility\n5.00\n0.242\n10616.365970\n13300000\n\n\n\n\n\n\n\n\nwind_df.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nonshore\n7.46\n31\n2\n0.433\n1.997163\n17495.14630\n7575.398348\n4.696747e+06\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nonshore\n7.45\n31\n10\n0.414\n9.945710\n87124.42376\n36069.511440\n2.236310e+07\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nonshore\n8.18\n31\n2\n0.506\n2.633037\n23065.40088\n11671.092850\n7.236078e+06\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nonshore\n7.17\n31\n16\n0.363\n14.185493\n124264.92160\n45108.166540\n2.796706e+07\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nonshore\n7.06\n31\n16\n0.358\n13.542570\n118632.91080\n42470.582050\n2.633176e+07\n\n\n\n\n\n\n\n\nsolar_df.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nirradiance\nlcoe\ncapacity\ncapacity_factor\narray_area\navailable_solar_resource\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nlarge_community\n5.634079\n39\n5.00\n0.235\n90633.862770\n21.276596\n6132.00\n4782960.0\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nsmall_utility\n5.616413\n39\n5.00\n0.234\n91307.484990\n21.367521\n6132.00\n4782960.0\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nsmall_community\n5.746738\n39\n0.15\n0.239\n2621.097459\n0.627615\n183.96\n143488.8\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nsmall_utility\n5.742196\n39\n5.00\n0.239\n87439.036330\n20.920502\n6132.00\n4782960.0\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nsmall_utility\n5.817187\n39\n5.00\n0.242\n85241.850210\n20.661157\n6132.00\n4782960.0"
  },
  {
    "objectID": "2-rf-wind.html",
    "href": "2-rf-wind.html",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree"
  },
  {
    "objectID": "2-rf-wind.html#imports",
    "href": "2-rf-wind.html#imports",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree"
  },
  {
    "objectID": "2-rf-wind.html#data-preprocessing",
    "href": "2-rf-wind.html#data-preprocessing",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nFirst, we read in the dataset.\n\ndf = pd.read_csv(\"../data/wind.csv\")\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nonshore\n7.46\n31\n2\n0.433\n1.997163\n17495.14630\n7575.398348\n4.696747e+06\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nonshore\n7.45\n31\n10\n0.414\n9.945710\n87124.42376\n36069.511440\n2.236310e+07\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nonshore\n8.18\n31\n2\n0.506\n2.633037\n23065.40088\n11671.092850\n7.236078e+06\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nonshore\n7.17\n31\n16\n0.363\n14.185493\n124264.92160\n45108.166540\n2.796706e+07\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nonshore\n7.06\n31\n16\n0.358\n13.542570\n118632.91080\n42470.582050\n2.633176e+07\n\n\n\n\n\n\n\nNow, we must shuffle the datasets to reduce bias.\n\ndf = df.sample(frac=1)\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n104321\n104321\n47.115517\n-106.994720\nMontana\nonshore\n8.08\n31\n16\n0.439\n20.301170\n177838.24560\n78070.98984\n48404013.70\n\n\n44108\n44108\n40.504593\n-87.896454\nIllinois\nonshore\n7.16\n36\n16\n0.374\n14.126223\n123745.70950\n46280.89535\n33322244.65\n\n\n19515\n19515\n36.364742\n-88.897675\nTennessee\nonshore\n6.74\n45\n16\n0.387\n11.783293\n103221.64420\n39946.77632\n35952098.69\n\n\n4940\n4940\n32.670708\n-102.887939\nTexas\nonshore\n7.34\n31\n16\n0.411\n15.218616\n133315.07550\n54792.49602\n33971347.53\n\n\n70776\n70776\n39.942379\n-122.230392\nCalifornia\nonshore\n6.11\n47\n16\n0.338\n8.778304\n76897.94144\n25991.50421\n24432013.95\n\n\n\n\n\n\n\nLooking at each dataset, we can identify which variables we want to use for our models.\n\nX = df.loc[:, ['lat','long','capacity']]\ny = df.loc[:, ['generated_energy','cost']]\n\nNow we split into training and testing sets, reserving about 80% for training and 20% for testing.\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_train = y[:100000]\ny_test = y[100000:]\n\nModels typically perform better when input values are within a certain range, like [-1, 1] for example. We scale the data points appropriately.\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train\n\narray([[ 1.50475958, -0.69192413,  0.51463162],\n       [-0.00765112,  0.91790859,  0.51463162],\n       [-0.95474337,  0.83351358,  0.51463162],\n       ...,\n       [ 0.20099693, -0.87590282, -0.07786604],\n       [-0.09467121, -0.45629192,  0.51463162],\n       [ 1.00200959,  0.73976282,  0.51463162]])"
  },
  {
    "objectID": "2-rf-wind.html#training-the-models",
    "href": "2-rf-wind.html#training-the-models",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "Training the Models",
    "text": "Training the Models\nNow that the data is pre-processed accordingly, the models can be trained and fit.\n\nreg = RandomForestRegressor()\nreg.fit(X_train, y_train)\n\nRandomForestRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor() \n\n\nWith a trained model, predictions can now be made.\n\ndisplay = y_test.reset_index()\npreds = reg.predict(X_test)\nprint(\"Predictions\")\nprint(\"----------------------\")\nfor i in range(3):\n    print(f\"predicted energy: {preds[i][0]:.2f}\\tactual energy: {display.at[i, 'generated_energy']:.2f}\\tpredicted cost: {preds[i][1]:.2f}\\tactual cost: {display.at[i, 'cost']:.2f}\")\n\nPredictions\n----------------------\npredicted energy: 39496.22  actual energy: 40307.25 predicted cost: 23697732.20 actual cost: 24184348.66\npredicted energy: 102419.64 actual energy: 101556.00    predicted cost: 61451784.90 actual cost: 60933600.27\npredicted energy: 49848.14  actual energy: 128958.46    predicted cost: 32899772.22 actual cost: 85112586.37"
  },
  {
    "objectID": "2-rf-wind.html#testing-and-analyzing-the-models",
    "href": "2-rf-wind.html#testing-and-analyzing-the-models",
    "title": "Random Forest Regression Process and Analysis for Wind Data",
    "section": "Testing and Analyzing the Models",
    "text": "Testing and Analyzing the Models\nThis section contains metrics gathering and other figures that visualize the models and its results.\n\nMetrics\n\nScores and Error Values\nThe score being recored are the R2 score, Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n\nr2 = metrics.r2_score(y_test, preds, multioutput=\"raw_values\")\nrmse = metrics.root_mean_squared_error(y_test, preds, multioutput=\"raw_values\")\nmape = metrics.mean_absolute_percentage_error(y_test, preds, multioutput=\"raw_values\")\n\nprint(\"Metric\\tScore\")\nprint(\"-----------------------\")\nprint(f\"r2\\t{r2}\\nrmse\\t{rmse}\\nmape\\t{mape}\")\n\nMetric  Score\n-----------------------\nr2  [0.9223821  0.89367722]\nrmse    [   8843.51723011 6421746.41274378]\nmape    [0.12902421 0.1288721 ]\n\n\n\n\nFeature Importances\nFeature importances give insights into the features that each decision tree in the random forest use to split most often. Results are portrayed in percentages.\n\nfeatures = ['lat','long','capacity',]\n\nimportances = reg.feature_importances_\nindices = np.argsort(importances)\n\nprint(\"Importances\")\nprint('----------------------')\nfor i in indices:\n    print(f\"{features[i]}: {importances[i]*100}\")\n\nImportances\n----------------------\ncapacity: 20.230960634451716\nlat: 35.095811124953094\nlong: 44.67322824059519\n\n\n\n\nK-Fold Cross Validation\nThis cross validation splits up the dataset into 10 unique folds, which are then used to test a model. The model is then scored using the same metrics outlined above: R2, RMSE, MAPE. This ensures the scoring is rigorous, and the entire dataset is used.\n\nkf = KFold(n_splits=10, random_state=0, shuffle=True)\nkf_cv_scores = cross_validate(reg, X, y, cv=kf, scoring={\"r2\":metrics.make_scorer(score_func=metrics.r2_score),\n \"rmse\":metrics.make_scorer(score_func=metrics.root_mean_squared_error),\n \"mape\":metrics.make_scorer(score_func=metrics.mean_absolute_percentage_error)})\nkf_cv_df = pd.DataFrame.from_dict(kf_cv_scores)\nmeans = kf_cv_df.mean()\nprint(\"10-Fold Cross Validation Scores\")\nprint(\"----------------------------------------------------\")\nprint(f\"R2 Average: {means.iloc[2]}\")\nprint(f\"RMSE Average: {means.iloc[3]}\")\nprint(f\"MAPE Average: {means.iloc[4]}\")\nkf_cv_df\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.9109908237527048\nRMSE Average: 3181739.2057383326\nMAPE Average: 0.13390596472645255\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n35.631838\n0.412091\n0.908636\n3.244508e+06\n0.126463\n\n\n1\n35.085633\n0.407737\n0.909902\n3.194235e+06\n0.134273\n\n\n2\n34.784758\n0.409132\n0.912724\n3.128678e+06\n0.129694\n\n\n3\n34.819360\n0.401673\n0.908216\n3.253428e+06\n0.126229\n\n\n4\n34.879103\n0.404808\n0.907344\n3.241619e+06\n0.137367\n\n\n5\n34.481580\n0.417363\n0.915569\n3.113150e+06\n0.137573\n\n\n6\n35.357341\n0.434344\n0.908085\n3.232867e+06\n0.148030\n\n\n7\n37.669082\n0.456003\n0.915380\n3.138264e+06\n0.137533\n\n\n8\n37.000147\n0.439912\n0.913334\n3.138594e+06\n0.128061\n\n\n9\n34.930129\n0.409387\n0.910719\n3.132047e+06\n0.133837\n\n\n\n\n\n\n\n\n\n\nGraphs\nGraphs of the Random Forest model fits on each of the input features, for each target.\n\nplot_lat_x = X[100000:].loc[:,['lat']].sort_values(by=['lat'])\nplot_long_x = X[100000:].loc[:,['long']].sort_values(by=['long'])\nplot_cap_x = X[100000:].loc[:,['capacity']].sort_values(by=['capacity'])\nplot_energy_y = pd.DataFrame(preds).loc[:,[0]].sort_values(by=[0])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Generated Energy(MWh)\")\naxis[0].set_title(\"Random Forest Regression: Generated Energy vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Generated Energy(MWh)\")\naxis[1].set_title(\"Random Forest Regression: Generated Energy vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Generated Energy(MWh)\")\naxis[2].set_title(\"Random Forest Regression: Generated Energy vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nplot_cost_y = pd.DataFrame(preds).loc[:,[1]].sort_values(by=[1])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Cost($)\")\naxis[0].set_title(\"Random Forest Regression: Cost vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Cost($)\")\naxis[1].set_title(\"Random Forest Regression: Cost vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Cost($)\")\naxis[2].set_title(\"Random Forest Regression: Cost vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nA graph of the feature importances. This helps to visualize the magnitude of importance of each feature, and compare their impact against one another.\n\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nA graph of one of the decision trees in the random forest. This displays the decision making process the model takes to arive at predictions.\n\nfn = ['lat','long','capacity']\ncn = ['generated_energy','cost']\nplt.subplots(nrows=1, ncols=1, figsize=(4,4), dpi=800)\ntree.plot_tree(reg.estimators_[0],feature_names=fn,class_names=cn,filled=True, max_depth=3)\nplt.show()\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "3-svm-wind.html",
    "href": "3-svm-wind.html",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "3-svm-wind.html#imports",
    "href": "3-svm-wind.html#imports",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "3-svm-wind.html#data-preprocessing",
    "href": "3-svm-wind.html#data-preprocessing",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nFirst, we read in the dataset.\n\ndf = pd.read_csv(\"../data/wind.csv\")\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nonshore\n7.46\n31\n2\n0.433\n1.997163\n17495.14630\n7575.398348\n4.696747e+06\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nonshore\n7.45\n31\n10\n0.414\n9.945710\n87124.42376\n36069.511440\n2.236310e+07\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nonshore\n8.18\n31\n2\n0.506\n2.633037\n23065.40088\n11671.092850\n7.236078e+06\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nonshore\n7.17\n31\n16\n0.363\n14.185493\n124264.92160\n45108.166540\n2.796706e+07\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nonshore\n7.06\n31\n16\n0.358\n13.542570\n118632.91080\n42470.582050\n2.633176e+07\n\n\n\n\n\n\n\nNow, we must shuffle the datasets to reduce bias.\n\ndf = df.sample(frac=1)\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n70816\n70816\n42.770447\n-105.846069\nWyoming\nonshore\n9.52\n30\n12\n0.429\n24.903425\n218154.00560\n93588.068410\n5.615284e+07\n\n\n45705\n45705\n40.527233\n-86.395111\nIndiana\nonshore\n7.38\n37\n16\n0.445\n15.468780\n135506.51270\n60300.398130\n4.462229e+07\n\n\n60161\n60161\n41.261482\n-82.054718\nOhio\nonshore\n6.94\n39\n10\n0.369\n8.039803\n70428.67401\n25988.180710\n2.027078e+07\n\n\n66924\n66924\n42.571953\n-88.600952\nWisconsin\nonshore\n7.65\n33\n16\n0.420\n17.229445\n150929.93620\n63390.573220\n4.183778e+07\n\n\n87265\n87265\n43.272633\n-112.698792\nIdaho\nonshore\n5.89\n44\n6\n0.306\n2.948928\n25832.60690\n7904.777711\n6.956204e+06\n\n\n\n\n\n\n\nLooking at each dataset, we can identify which variables we want to use for our models.\n\nX = df.loc[:, ['lat','long','capacity']]\ny_energy = df['generated_energy'].values\ny_cost = df['cost'].values\n\nNow we split into training and testing sets, reserving about 80% for training and 20% for testing.\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_energy_train = y_energy[:100000]\ny_energy_test = y_energy[100000:]\ny_cost_train = y_cost[:100000]\ny_cost_test = y_cost[100000:]\n\nModels typically perform better when input values are within a certain range, like [-1, 1] for example. We scale the data points appropriately.\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train\n\narray([[ 5.12792434e-01, -5.97749762e-01, -6.74179171e-01],\n       [-5.40909232e-04,  1.04045828e+00,  5.14335794e-01],\n       [ 1.67483405e-01,  1.40601696e+00, -1.26843665e+00],\n       ...,\n       [ 7.18263624e-01,  2.25951248e+00, -1.26843665e+00],\n       [ 1.85068016e+00, -6.61101747e-01,  5.14335794e-01],\n       [ 2.41293149e-01, -6.99414135e-01,  5.14335794e-01]])"
  },
  {
    "objectID": "3-svm-wind.html#training-the-models",
    "href": "3-svm-wind.html#training-the-models",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "Training the Models",
    "text": "Training the Models\nNow that the data is pre-processed accordingly, the models can be trained and fit.\n\nenergy_reg = svm.SVR()\ncost_reg = svm.SVR()\nenergy_reg.fit(X_train, y_energy_train)\ncost_reg.fit(X_train, y_cost_train)\n\nSVR()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVR?Documentation for SVRiFittedSVR() \n\n\nWith a trained model, predictions can now be made.\n\nenergy_display = y_energy_test\ncost_display = y_cost_test\nenergy_preds = energy_reg.predict(X_test)\ncost_preds = cost_reg.predict(X_test)\nprint(\"Predictions\")\nprint(\"----------------------\")\nfor i in range(3):\n    print(f\"predicted energy: {energy_preds[i]:.2f}\\tactual energy: {energy_display[i]:.2f}\\tpredicted cost: {cost_preds[i]:.2f}\\tactual cost: {cost_display[i]:.2f}\")\n\nPredictions\n----------------------\npredicted energy: 57328.67  actual energy: 40842.41 predicted cost: 39489716.81 actual cost: 36758169.87\npredicted energy: 49636.20  actual energy: 10659.56 predicted cost: 39480504.51 actual cost: 6822119.29\npredicted energy: 50298.13  actual energy: 9386.53  predicted cost: 39481146.84 actual cost: 8823336.22"
  },
  {
    "objectID": "3-svm-wind.html#testing-and-analyzing-the-models",
    "href": "3-svm-wind.html#testing-and-analyzing-the-models",
    "title": "Support Vector Regression Process and Analysis for Wind Data",
    "section": "Testing and Analyzing the Models",
    "text": "Testing and Analyzing the Models\nThis section contains metrics gathering and other figures that visualize the models and its results.\n\nMetrics\n\nScores and Error Values\nThe score being recored are the R2 score, Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n\nenergy_r2 = metrics.r2_score(y_energy_test, energy_preds, multioutput=\"raw_values\")\nenergy_rmse = metrics.root_mean_squared_error(y_energy_test, energy_preds, multioutput=\"raw_values\")\nenergy_mape = metrics.mean_absolute_percentage_error(y_energy_test, energy_preds, multioutput=\"raw_values\")\n\ncost_r2 = metrics.r2_score(y_cost_test, cost_preds, multioutput=\"raw_values\")\ncost_rmse = metrics.root_mean_squared_error(y_cost_test, cost_preds, multioutput=\"raw_values\")\ncost_mape = metrics.mean_absolute_percentage_error(y_cost_test, cost_preds, multioutput=\"raw_values\")\n\nprint(\"Metric\\tScore\")\nprint(\"-----------------------\")\nprint(f\"energy_r2\\t{energy_r2}\\ncost_r2\\t{cost_r2}\\nenergy_rmse\\t{energy_rmse}\\ncost_rmse\\t{cost_rmse}\\nenergy_mape\\t{energy_mape}\\ncost_mape\\t{cost_mape}\")\n\nMetric  Score\n-----------------------\nenergy_r2   [0.123724]\ncost_r2 [0.00020249]\nenergy_rmse [29742.00648558]\ncost_rmse   [19613745.35965985]\nenergy_mape [1.08458669]\ncost_mape   [0.98070773]\n\n\n\n\nK-Fold Cross Validation\nThis cross validation splits up the dataset into 10 unique folds, which are then used to test a model. The model is then scored using the same metrics outlined above: R2, RMSE, MAPE. This ensures the scoring is rigorous, and the entire dataset is used.\n\nkf = KFold(n_splits=10, random_state=0, shuffle=True)\nkf_cv_scores = cross_validate(energy_reg, X, y_energy, cv=kf, scoring={\"r2\":metrics.make_scorer(score_func=metrics.r2_score),\n \"rmse\":metrics.make_scorer(score_func=metrics.root_mean_squared_error),\n \"mape\":metrics.make_scorer(score_func=metrics.mean_absolute_percentage_error)})\nkf_cv_df = pd.DataFrame.from_dict(kf_cv_scores)\nmeans = kf_cv_df.mean()\nprint(\"10-Fold Cross Validation Scores\")\nprint(\"----------------------------------------------------\")\nprint(f\"R2 Average: {means.iloc[2]}\")\nprint(f\"RMSE Average: {means.iloc[3]}\")\nprint(f\"MAPE Average: {means.iloc[4]}\")\nkf_cv_df\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.0018450300789964902\nRMSE Average: 31730.72214388117\nMAPE Average: 1.1974198375900589\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n771.961335\n90.775717\n0.002921\n31909.960130\n1.150887\n\n\n1\n771.080194\n90.634640\n0.002406\n31278.957484\n1.194819\n\n\n2\n758.963423\n90.289549\n0.003153\n31580.604349\n1.276420\n\n\n3\n740.477523\n88.983850\n0.001673\n31700.498088\n1.158536\n\n\n4\n736.539896\n88.923703\n0.001296\n31842.196502\n1.261852\n\n\n5\n737.672136\n89.011240\n0.000597\n31694.965905\n1.201443\n\n\n6\n736.014961\n89.161244\n0.002357\n31440.657336\n1.128294\n\n\n7\n738.921449\n88.798775\n0.002150\n31583.352399\n1.273676\n\n\n8\n738.560741\n88.798225\n0.000881\n32331.064876\n1.176411\n\n\n9\n737.216899\n88.766471\n0.001017\n31944.964370\n1.151860\n\n\n\n\n\n\n\n\nkf = KFold(n_splits=10, random_state=0, shuffle=True)\nkf_cv_scores = cross_validate(cost_reg, X, y_cost, cv=kf, scoring={\"r2\":metrics.make_scorer(score_func=metrics.r2_score),\n \"rmse\":metrics.make_scorer(score_func=metrics.root_mean_squared_error),\n \"mape\":metrics.make_scorer(score_func=metrics.mean_absolute_percentage_error)})\nkf_cv_df = pd.DataFrame.from_dict(kf_cv_scores)\nmeans = kf_cv_df.mean()\nprint(\"10-Fold Cross Validation Scores\")\nprint(\"----------------------------------------------------\")\nprint(f\"R2 Average: {means.iloc[2]}\")\nprint(f\"RMSE Average: {means.iloc[3]}\")\nprint(f\"MAPE Average: {means.iloc[4]}\")\nkf_cv_df\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: -0.00017107792075631288\nRMSE Average: 19730288.835201103\nMAPE Average: 0.9651219794341653\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n401.449423\n88.838005\n-0.000394\n1.963845e+07\n0.926044\n\n\n1\n401.821544\n89.089334\n-0.000016\n1.952591e+07\n0.965267\n\n\n2\n400.949044\n88.836812\n-0.000417\n1.976890e+07\n1.021886\n\n\n3\n401.101503\n88.819128\n-0.000002\n1.964222e+07\n0.946757\n\n\n4\n400.615725\n88.751960\n-0.000103\n1.992799e+07\n1.014057\n\n\n5\n401.312933\n89.032375\n-0.000370\n1.971245e+07\n0.962284\n\n\n6\n401.158882\n88.770521\n-0.000006\n1.930747e+07\n0.919042\n\n\n7\n400.537041\n88.876982\n0.000005\n1.968026e+07\n1.019527\n\n\n8\n400.704807\n88.816629\n-0.000183\n2.014325e+07\n0.954541\n\n\n9\n407.294723\n88.969792\n-0.000225\n1.995599e+07\n0.921815\n\n\n\n\n\n\n\n\nGraphs\nGraphs of the SVR model fits on each on the input features, for each target.\n\nplot_lat_x = X[100000:].loc[:,['lat']].sort_values(by=['lat'])\nplot_long_x = X[100000:].loc[:,['long']].sort_values(by=['long'])\nplot_cap_x = X[100000:].loc[:,['capacity']].sort_values(by=['capacity'])\nplot_energy_y = pd.DataFrame(energy_preds).loc[:,[0]].sort_values(by=[0])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y_energy, color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Generated Energy(MWh)\")\naxis[0].set_title(\"Support Vector Regression: Generated Energy vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y_energy, color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Generated Energy(MWh)\")\naxis[1].set_title(\"Support Vector Regression: Generated Energy vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y_energy, color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Generated Energy(MWh)\")\naxis[2].set_title(\"Support Vector Regression: Generated Energy vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nplot_cost_y = pd.DataFrame(cost_preds).loc[:,[0]].sort_values(by=[0])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y_cost, color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Cost($)\")\naxis[0].set_title(\"Support Vector Regression: Cost vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y_cost, color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Cost($)\")\naxis[1].set_title(\"Support Vector Regression: Cost vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y_cost, color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Cost($)\")\naxis[2].set_title(\"Support Vector Regression: Cost vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "4-ann-wind.html",
    "href": "4-ann-wind.html",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor"
  },
  {
    "objectID": "4-ann-wind.html#imports",
    "href": "4-ann-wind.html#imports",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor"
  },
  {
    "objectID": "4-ann-wind.html#data-preprocessing",
    "href": "4-ann-wind.html#data-preprocessing",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nFirst, we read in the dataset.\n\ndf = pd.read_csv(\"../data/wind.csv\")\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nonshore\n7.46\n31\n2\n0.433\n1.997163\n17495.14630\n7575.398348\n4.696747e+06\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nonshore\n7.45\n31\n10\n0.414\n9.945710\n87124.42376\n36069.511440\n2.236310e+07\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nonshore\n8.18\n31\n2\n0.506\n2.633037\n23065.40088\n11671.092850\n7.236078e+06\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nonshore\n7.17\n31\n16\n0.363\n14.185493\n124264.92160\n45108.166540\n2.796706e+07\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nonshore\n7.06\n31\n16\n0.358\n13.542570\n118632.91080\n42470.582050\n2.633176e+07\n\n\n\n\n\n\n\nNow, we must shuffle the datasets to reduce bias.\n\ndf = df.sample(frac=1)\ndf.head(5)\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n79744\n79744\n43.146877\n-82.945618\nMichigan\nonshore\n7.18\n32\n16\n0.431\n14.244930\n124785.58400\n53782.58670\n34420855.49\n\n\n97161\n97161\n45.597183\n-104.014069\nSouth Dakota\nonshore\n7.69\n30\n16\n0.396\n17.501126\n153309.86550\n60710.70672\n36426424.03\n\n\n979\n979\n29.375134\n-90.818909\nLouisiana\nonshore\n6.15\n50\n16\n0.307\n8.951840\n78418.12075\n24074.36307\n24074363.07\n\n\n110376\n110376\n48.992310\n-101.517090\nNorth Dakota\nonshore\n7.44\n30\n16\n0.408\n15.849143\n138838.49260\n56646.10498\n33987662.99\n\n\n46495\n46495\n40.798595\n-88.867004\nIllinois\nonshore\n7.34\n36\n14\n0.405\n13.316289\n116650.69100\n47243.52987\n34015341.51\n\n\n\n\n\n\n\nLooking at each dataset, we can identify which variables we want to use for our models.\n\nX = df.loc[:, ['lat','long','capacity']]\ny = df.loc[:, ['generated_energy','cost']]\n\nNow we split into training and testing sets, reserving about 80% for training and 20% for testing.\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_train = y[:100000]\ny_test = y[100000:]\n\nModels typically perform better when input values are within a certain range, like [-1, 1] for example. We scale the data points appropriately.\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train\n\narray([[ 0.59757844,  1.33120197,  0.51419331],\n       [ 1.15920501, -0.44453102,  0.51419331],\n       [-2.55899743,  0.66760964,  0.51419331],\n       ...,\n       [ 0.28400189, -0.71009965,  0.51419331],\n       [-0.42856715, -1.06174499, -0.07960454],\n       [-1.26252269, -0.57359601, -0.07960454]])"
  },
  {
    "objectID": "4-ann-wind.html#training-the-models",
    "href": "4-ann-wind.html#training-the-models",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "Training the Models",
    "text": "Training the Models\nNow that the data is pre-processed accordingly, the models can be trained and fit.\n\nreg = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(4,), max_iter=100000000000)\nreg.fit(X_train, y_train)\n\nMLPRegressor(hidden_layer_sizes=(4,), max_iter=100000000000, solver='lbfgs')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MLPRegressor?Documentation for MLPRegressoriFittedMLPRegressor(hidden_layer_sizes=(4,), max_iter=100000000000, solver='lbfgs') \n\n\nWith a trained model, predictions can now be made.\n\ndisplay = y_test.reset_index()\npreds = reg.predict(X_test)\nprint(\"Predictions\")\nprint(\"----------------------\")\nfor i in range(3):\n    print(f\"predicted energy: {preds[i][0]:.2f}\\tactual energy: {display.at[i, 'generated_energy']:.2f}\\tpredicted cost: {preds[i][1]:.2f}\\tactual cost: {display.at[i, 'cost']:.2f}\")\n\nPredictions\n----------------------\npredicted energy: 67701.71  actual energy: 24145.95 predicted cost: 45421920.70 actual cost: 28492225.26\npredicted energy: 51943.76  actual energy: 34637.88 predicted cost: 36780099.30 actual cost: 22860998.02\npredicted energy: 65571.38  actual energy: 66479.65 predicted cost: 43653964.82 actual cost: 39887787.57"
  },
  {
    "objectID": "4-ann-wind.html#testing-and-analyzing-the-models",
    "href": "4-ann-wind.html#testing-and-analyzing-the-models",
    "title": "Artificial Neural Network Process and Analysis for Wind Data",
    "section": "Testing and Analyzing the Models",
    "text": "Testing and Analyzing the Models\nThis section contains metrics gathering and other figures that visualize the models and its results.\n\nMetrics\n\nScores and Error Values\nThe score being recored are the R2 score, Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n\nr2 = metrics.r2_score(y_test, preds, multioutput=\"raw_values\")\nrmse = metrics.root_mean_squared_error(y_test, preds, multioutput=\"raw_values\")\nmape = metrics.mean_absolute_percentage_error(y_test, preds, multioutput=\"raw_values\")\n\nprint(\"Metric\\tScore\")\nprint(\"-----------------------\")\nprint(f\"r2\\t{r2}\\nrmse\\t{rmse}\\nmape\\t{mape}\")\n\nMetric  Score\n-----------------------\nr2  [0.19333991 0.19953254]\nrmse    [   28635.94093632 17726324.27849805]\nmape    [0.80743714 0.62410014]\n\n\n\n\nK-Fold Cross Validation\nThis cross validation splits up the dataset into 10 unique folds, which are then used to test a model. The model is then scored using the same metrics outlined above: R2, RMSE, MAPE. This ensures the scoring is rigorous, and the entire dataset is used.\n\nkf = KFold(n_splits=10, shuffle=True)\nkf_cv_scores = cross_validate(reg, X, y, cv=kf, scoring={\"r2\":metrics.make_scorer(score_func=metrics.r2_score),\n \"rmse\":metrics.make_scorer(score_func=metrics.root_mean_squared_error),\n \"mape\":metrics.make_scorer(score_func=metrics.mean_absolute_percentage_error)})\nkf_cv_df = pd.DataFrame.from_dict(kf_cv_scores)\nmeans = kf_cv_df.mean()\nprint(\"10-Fold Cross Validation Scores\")\nprint(\"----------------------------------------------------\")\nprint(f\"R2 Average: {means.iloc[2]}\")\nprint(f\"RMSE Average: {means.iloc[3]}\")\nprint(f\"MAPE Average: {means.iloc[4]}\")\nkf_cv_df\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.17550572322297842\nRMSE Average: 8955297.046558464\nMAPE Average: 0.7180028050220215\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n2.288576\n0.003992\n0.189615\n8.928638e+06\n0.654504\n\n\n1\n0.733644\n0.004988\n0.191056\n8.854147e+06\n0.730762\n\n\n2\n0.715144\n0.003999\n0.186500\n8.930511e+06\n0.652372\n\n\n3\n3.342753\n0.004004\n0.196500\n8.899258e+06\n0.715955\n\n\n4\n1.022440\n0.012998\n0.196300\n8.833933e+06\n0.677797\n\n\n5\n7.003229\n0.003998\n0.201263\n8.808617e+06\n0.666556\n\n\n6\n0.753272\n0.004013\n-0.000947\n9.860140e+06\n1.071770\n\n\n7\n0.807899\n0.004000\n0.200553\n8.879911e+06\n0.647242\n\n\n8\n0.673103\n0.003997\n0.204381\n8.752960e+06\n0.685099\n\n\n9\n1.356002\n0.002999\n0.189836\n8.804856e+06\n0.677972\n\n\n\n\n\n\n\n\n\n\nGraphs\nGraphs of the neural network fits on each of the input features, for each target.\n\nplot_lat_x = X[100000:].loc[:,['lat']].sort_values(by=['lat'])\nplot_long_x = X[100000:].loc[:,['long']].sort_values(by=['long'])\nplot_cap_x = X[100000:].loc[:,['capacity']].sort_values(by=['capacity'])\nplot_energy_y = pd.DataFrame(preds).loc[:,[0]].sort_values(by=[0])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Generated Energy(MWh)\")\naxis[0].set_title(\"Artificial Neural Network: Generated Energy vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Generated Energy(MWh)\")\naxis[1].set_title(\"Artificial Neural Network: Generated Energy vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y.loc[:,['generated_energy']], color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_energy_y, color='red',lw=2, label=\"Generated Energy Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Generated Energy(MWh)\")\naxis[2].set_title(\"Artificial Neural Network: Generated Energy vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nplot_cost_y = pd.DataFrame(preds).loc[:,[1]].sort_values(by=[1])\n\nfigure, axis = plt.subplots(3)\n\nfigure.set_size_inches(15,15)\n\naxis[0].scatter(X.loc[:,[\"lat\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[0].plot(plot_lat_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[0].set_xlabel(\"Latitude\")\naxis[0].set_ylabel(\"Cost($)\")\naxis[0].set_title(\"Artificial Neural Network: Cost vs. Latitude\")\naxis[0].legend()\n\naxis[1].scatter(X.loc[:,[\"long\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[1].plot(plot_long_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[1].set_xlabel(\"Longitude\")\naxis[1].set_ylabel(\"Cost($)\")\naxis[1].set_title(\"Artificial Neural Network: Cost vs. Longitude\")\naxis[1].legend()\n\naxis[2].scatter(X.loc[:,[\"capacity\"]], y.loc[:,['cost']], color='blue', label='Data', s=5)\naxis[2].plot(plot_cap_x, plot_cost_y, color='red',lw=2, label=\"Cost Model\")\naxis[2].set_xlabel(\"Capacity(MW)\")\naxis[2].set_ylabel(\"Cost($)\")\naxis[2].set_title(\"Artificial Neural Network: Cost vs. Capacity\")\naxis[2].legend()\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=.4, \n                    hspace=.4)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nA Graph that models the structure of a nueral network as a visual aid.\n\n# Code originally comes from @craffel on GitHub: https://gist.github.com/craffel/2d727968c3aaebd10359\n# Updates were made to make it compatible with Python 3\ndef draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n    '''\n    Draw a neural network cartoon using matplotilb.\n    \n    :usage:\n        &gt;&gt;&gt; fig = plt.figure(figsize=(12, 12))\n        &gt;&gt;&gt; draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n    \n    :parameters:\n        - ax : matplotlib.axes.AxesSubplot\n            The axes on which to plot the cartoon (get e.g. by plt.gca())\n        - left : float\n            The center of the leftmost node(s) will be placed here\n        - right : float\n            The center of the rightmost node(s) will be placed here\n        - bottom : float\n            The center of the bottommost node(s) will be placed here\n        - top : float\n            The center of the topmost node(s) will be placed here\n        - layer_sizes : list of int\n            List of layer sizes, including input and output dimensionality\n    '''\n    n_layers = len(layer_sizes)\n    v_spacing = (top - bottom)/float(max(layer_sizes))\n    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n    # Nodes\n    for n, layer_size in enumerate(layer_sizes):\n        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n        for m in range(layer_size):\n            circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/4.,\n                                color='w', ec='k', zorder=4)\n            ax.add_artist(circle)\n    # Edges\n    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n        for m in range(layer_size_a):\n            for o in range(layer_size_b):\n                line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n                                  [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n                ax.add_artist(line)\n\nfig = plt.figure(figsize=(12, 12))\nax = fig.gca()\nax.axis('off')\ndraw_neural_net(ax, .1, .9, .1, .9, [3, 4, 2])\n\nplt.text(.06, .80, \"Input Layer\")\nplt.text(.45, .90, \"Hidden Layer\")\nplt.text(.85, .70, \"Output Layer\")\n\nplt.show()\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "6-data-clustering.html",
    "href": "6-data-clustering.html",
    "title": "Data Clustering",
    "section": "",
    "text": "Data Clustering Example\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n\ndf = pd.read_csv('../data/wind_old.csv')\nX = df.loc[:, df.columns[1:3]]\n\n\nY_axis = X[['lat']]\nX_axis = X[['long']]\nK_clusters = range(1,100)\nkmeans = [KMeans(n_clusters=i, n_init='auto') for i in K_clusters]\nscore = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]\n\n\nplt.plot(K_clusters, score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nn_clusters = 20\nkmeans = KMeans(n_clusters, init='k-means++', n_init='auto')\nkmeans.fit(X[X.columns[0:2]])\n\nKMeans(n_clusters=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=20) \n\n\n\nX['cluster_label'] = kmeans.fit_predict(X[X.columns[0:2]])\nX.head(5)\n\n\n\n\n\n\n\n\nlat\nlong\ncluster_label\n\n\n\n\n0\n23.510410\n-117.147260\n7\n\n\n1\n24.007446\n-93.946777\n6\n\n\n2\n25.069138\n-97.482483\n6\n\n\n3\n25.069443\n-97.463135\n6\n\n\n4\n25.069763\n-97.443756\n6\n\n\n\n\n\n\n\n\ncenters = kmeans.cluster_centers_\nlabels = kmeans.predict(X[X.columns[0:2]])\nX.plot.scatter(x = 'long', y = 'lat', c=labels, s=50, cmap='viridis')\nplt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "A Look at the Data",
    "section": "",
    "text": "Data are a crucial piece of this research project. As such, much of the effort and consideration went into selecting, crafting, and refactoring the datasets to suit the need as best as possible. Throughout this process, many things were tried and tossed away. These include the form of the data, and even methods for processing the data to make it easier for certain machine learning models to use them. In the end, an effective appraoch was discovered. Each of these snapshots will be covered below.\n\n\nBoth the wind and solar data used in this project orginally come from The National Renewable Energy Laboratory (NREL). The data comes with many attributes that are crucial to the analysis this project seeks to conduct, however, there are still pieces missing to truly acheive the full picture. Regardless, compared to other publically available datasets, these were the best suited for the job, and offered a good starting point.\n\n\nThe original datasets came with some basic attributes:\n\nlongitude\nlatitude\nwind speed\nsolar irradiance\ncapacity\ncapacity factor\n\nThis, at first, is not enough data to begin predicting for energy generation and cost. For this, actual data on energy generation and cost is needed. With what was provided, these could be calculated. To start, a few simple calculations were used to find the energy and cost values desired. The attribute capacity denotes how much energy a certain technology can produce under ideal conditions. Capacity factor denotes what fraction of the technology’s capacity is produced. With this, a simple estimate for energy generation can be calculated by multiplying both values together. For cost, many estimates exist that outline how much a technology costs per watt of installed capacity. Using these estimates, and multiplying by capacity, a value for cost can be aquired. This produced two datasets that took this shape:\n\n\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nwind_speed\nfarm_type\ncapacity\ncapacity_factor\npower_generation\nestimated_cost\n\n\n\n\n0\n0\n23.510410\n-117.147260\n6.07\noffshore\n16\n0.169\n23687.04\n20800000\n\n\n1\n1\n24.007446\n-93.946777\n7.43\noffshore\n16\n0.302\n42328.32\n20800000\n\n\n2\n2\n25.069138\n-97.482483\n8.19\noffshore\n16\n0.375\n52560.00\n20800000\n\n\n3\n3\n25.069443\n-97.463135\n8.19\noffshore\n16\n0.375\n52560.00\n20800000\n\n\n4\n4\n25.069763\n-97.443756\n8.19\noffshore\n16\n0.376\n52700.16\n20800000\n\n\n\n\n\n\n\nSource: Data Preview\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nirradiance\nfarm_type\ncapacity\ncapacity_factor\npower_generation\nestimated_cost\n\n\n\n\n0\n0\n25.896492\n-97.460358\n5.634079\nlarge_community\n5.00\n0.235\n10282.193270\n13300000\n\n\n1\n1\n26.032654\n-97.738098\n5.616413\nsmall_utility\n5.00\n0.234\n10249.953070\n13300000\n\n\n2\n2\n26.059063\n-97.208252\n5.746738\nsmall_community\n0.15\n0.239\n314.633929\n399000\n\n\n3\n3\n26.078449\n-98.073364\n5.742196\nsmall_utility\n5.00\n0.239\n10479.506980\n13300000\n\n\n4\n4\n26.143227\n-98.311340\n5.817187\nsmall_utility\n5.00\n0.242\n10616.365970\n13300000\n\n\n\n\n\n\n\nSource: Data Preview\nThe rough and simplistic nature of these estimates are generally okay as the concern of this project is to determine how well machine learning algorithms predict values, and not the correctness and integrity of the values themselves. Nonetheless, issues still appeared because of this. When training and testing the Random Forest Regressor, a feature importance graph was used to determine which attributes are the most important. This can also give insights into how the variables are used by other models as well. When running on the old wind data, the graph indicated one variable was dominating the dataset.\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nSource: Old Random Forest Model\n\n\n\nFeature Importances\n----------------------------\nlat: 1.7415146179729832e-09\nlong: 4.556724073707158e-09\nwind_speed: 2.5874951940190983e-08\ncapacity_factor: 0.0005676720796435423\ncapacity: 99.99943229574717\n\n\nSource: Old Random Forest Model\nOut of all of the features, capacity was shown to have the highest importance. This makes sense as both calculations are made up of direct proportions that scale based on capacity. This then indicates that the issue is not Random Forest exclusive, as this bias is present in the dataset, and not in how the algorithm interprets the dataset. Changes had to be implemeneted to remove this bias, which included adding more features and using more in-depth calculations to derive the features to be predicted. These are covered in The New Data.\n\n\n\n\nThe initial direction of this project was to look specifically into clustering techniques and cluster analysis. This process consists of grouping similar data into a specified number of clusters. When considering the geospatial data used for this project, clustering would be used to abstract coordinates in the form of longitude and latitude into a single value, a cluster ID. The cluster ID’s would then represent the other features of the data point, allowing for predictions of other features to be made based off of a given cluster ID or coordinate location. The main two steps for performing the clustering is breifly outlined below:\n\nDetermine the number of optimal clusters for the dataset.\n\nTo do this, you just need to cluster the data with varying cluster numbers, score them, and graph the scores by the number of clusters and determine the point at which gains start to diminish.\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nSource: Data Clustering\n\nCluster the data and plot the results\n\nWith the optimal number of clusters (or any desired number), the data can be clustered, with each data point given a cluster ID.\n\n\n\n\n\n\n\n\n\n\nlat\nlong\ncluster_label\n\n\n\n\n0\n23.510410\n-117.147260\n7\n\n\n1\n24.007446\n-93.946777\n6\n\n\n2\n25.069138\n-97.482483\n6\n\n\n3\n25.069443\n-97.463135\n6\n\n\n4\n25.069763\n-97.443756\n6\n\n\n\n\n\n\n\nSource: Data Clustering\nNow that clusters are generated, they can be plotted and visualized.\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nSource: Data Clustering\nThis not only shows the shape and dimensions of the data, but it also gives insights into how many clusters are suitable for the task. The scale of the data makes the number of optimal clusters here a bit ambiguous. Although around 20 seems to be the best, 20 clusters does not seem to represent the nuances of how geospatial data like wind speed and solar raditation changes throughout the country. This seems to indicate that this approach is a bit unreliable in that regard and could introduce more bias into the data and models.\nAdditionally, the clustering could also help increase the performance of the algorithms by lowering the number of input parameters. Simplifying the inputs allows the models to interpret the data easier, leading to better predictions. The issues described in the structure and make-up section further examplified this potential benefit. But for those same reasons, clustering ended up not being an ideal pathway forward. Not only would refactoring the data to remove the bias negate the benefits of clustering in the long run, but clustering also ends up become computationally expensive and tedious to implement and analyze given the nature of the data. For these reasons it was discarded as an aspect of the project.\n\n\n\n\nTo overcome the issues outlined above, the datasets were both refactored heavily to remove the bias present. This fixed structural issues, but also allowed for in-depth analysis of mahcine learning models without the time and efficiency overheads that would be present if clustering remained a crucial piece of the project. The process and results of refactoring both of the datasets are outlined below:\n\n\nTo reduce the bias in the wind dataset, new features were added and a new equation was used that incorporated more features than just capacity to ensure a wider range of importance across the board. Two new features were added:\n\nState - the state that the latitude and longitude coordinates are within.\nLevelized Cost of Energy (LCOE) - the average net cost of electricity generation for a generator over its lifetime.\n\nBoth of these features gave way for more inclusive calculations to be used for both energy generation and cost. The updated method to find the energy generation, \\(J\\), for a given wind farm uses the following equation where \\(c\\) is capacity, \\(\\rho\\) is the density of air, \\(A\\) is the swept area of the wind turbine, \\(v\\) is the wind speed, \\(h\\) is the number of hours in a year, and \\(c_f\\) is capacity factor:\n\n\\(J = \\frac{c}{2}\\left( \\frac{1}{2}\\rho Av^3\\right)h c_f\\)\nSome values here are assumed constants: \\(\\rho = 1.225 kg/m^3\\), \\(A = 7854 m^2\\), \\(h = 8760hrs\\)\n\nAlso, this equation was split at certain parts to increase the number of features, to aide the model in being able to track the relationships better. The splits added the features available_wind_power and available_energy.\nCalculating the cost using LCOE is very simple. A technology’s LCOE is measured in dollars per watt-hour of energy generated. The only other piece needed is the lifespan of the technology. For this project, a 20 year lifespan is assumed for all wind turbines. This results in this equation where \\(J\\) is generated energy, \\(l\\) is the LCOE of the technology in the state, and \\(t\\) is the lifespan of the technology:\n\n\\(C = Jlt\\)\n\nThese equations incorporate crucial pieces of geospatial data that were not present in the previous calculation. It also adds other assumed constants, which add complexity to the calculation and obscure the relationships a bit, giving the model more agency to work out the connections. In the end, it results in a dataset that looks like this:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nonshore\n7.46\n31\n2\n0.433\n1.997163\n17495.14630\n7575.398348\n4.696747e+06\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nonshore\n7.45\n31\n10\n0.414\n9.945710\n87124.42376\n36069.511440\n2.236310e+07\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nonshore\n8.18\n31\n2\n0.506\n2.633037\n23065.40088\n11671.092850\n7.236078e+06\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nonshore\n7.17\n31\n16\n0.363\n14.185493\n124264.92160\n45108.166540\n2.796706e+07\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nonshore\n7.06\n31\n16\n0.358\n13.542570\n118632.91080\n42470.582050\n2.633176e+07\n\n\n\n\n\n\n\nSource: Data Preview\nThis dataset can be accessed and downloaded from here.\n\n\n\nSimilar to the wind dataset, bias was reduced by adding more features and using a new equation which incorporated more features. Like the wind data, to make these adjustments, state and lcoe needed to be added as features. On top of these, two other features were dervied as part of the calculation to determine the generated energy. These are as follows:\n\nArray Area, where \\(c\\) is capacity, \\(I\\) is the solar irradiance and \\(c_f\\) is the capacity factor.\n\n\n\\(A = \\frac{24c}{Ic_f}\\)\nWe multiply by 24 because \\(I\\) is in units of \\(kWh/m^2/day\\) and we want it in units of \\(kWh/m^2\\) to ensure we end up with an area as the result.\n\n\nAvailable Solar Resource, where \\(I\\) is the solar irradiance and \\(A\\) is the array area.\n\n\n\\(P = \\frac{IA}{24*1000}\\)\nThe 24 serves the same purpose as before, but the 1000 here changes the units from \\(kWh\\) to \\(MWh\\) to stay consistent all results.\n\nWith these, an equation for generated energy, \\(J\\) can be derived, where \\(P\\) is the available solar resource, \\(h\\) is the number of hours in a year, \\(c_f\\) is the capacity factor, and \\(q\\) is the system losses, which is assumed to be 14%:\n\n\\(J = P h  c_f  q\\)\n\nSame as the wind data, calculating the cost using the LCOE is very simple. A lifespan of 20 years is used again for consistency:\n\n\\(C = Jlt\\)\n\nThe chain of calculations incorporate more of the geospatial data, which exemplifies the purpose of the research project much more than what the old data had, even with clustering. Again, it allows the model to do its best at finding the relationship, compared to just recognizing a direct proportion like before. The resulting solar dataset looks like this:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nirradiance\nlcoe\ncapacity\ncapacity_factor\narray_area\navailable_solar_resource\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nlarge_community\n5.634079\n39\n5.00\n0.235\n90633.862770\n21.276596\n6132.00\n4782960.0\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nsmall_utility\n5.616413\n39\n5.00\n0.234\n91307.484990\n21.367521\n6132.00\n4782960.0\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nsmall_community\n5.746738\n39\n0.15\n0.239\n2621.097459\n0.627615\n183.96\n143488.8\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nsmall_utility\n5.742196\n39\n5.00\n0.239\n87439.036330\n20.920502\n6132.00\n4782960.0\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nsmall_utility\n5.817187\n39\n5.00\n0.242\n85241.850210\n20.661157\n6132.00\n4782960.0\n\n\n\n\n\n\n\nSource: Data Preview\nThis dataset is also available to be accessed and downloaded here."
  },
  {
    "objectID": "data.html#the-old-data",
    "href": "data.html#the-old-data",
    "title": "A Look at the Data",
    "section": "",
    "text": "Both the wind and solar data used in this project orginally come from The National Renewable Energy Laboratory (NREL). The data comes with many attributes that are crucial to the analysis this project seeks to conduct, however, there are still pieces missing to truly acheive the full picture. Regardless, compared to other publically available datasets, these were the best suited for the job, and offered a good starting point.\n\n\nThe original datasets came with some basic attributes:\n\nlongitude\nlatitude\nwind speed\nsolar irradiance\ncapacity\ncapacity factor\n\nThis, at first, is not enough data to begin predicting for energy generation and cost. For this, actual data on energy generation and cost is needed. With what was provided, these could be calculated. To start, a few simple calculations were used to find the energy and cost values desired. The attribute capacity denotes how much energy a certain technology can produce under ideal conditions. Capacity factor denotes what fraction of the technology’s capacity is produced. With this, a simple estimate for energy generation can be calculated by multiplying both values together. For cost, many estimates exist that outline how much a technology costs per watt of installed capacity. Using these estimates, and multiplying by capacity, a value for cost can be aquired. This produced two datasets that took this shape:\n\n\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nwind_speed\nfarm_type\ncapacity\ncapacity_factor\npower_generation\nestimated_cost\n\n\n\n\n0\n0\n23.510410\n-117.147260\n6.07\noffshore\n16\n0.169\n23687.04\n20800000\n\n\n1\n1\n24.007446\n-93.946777\n7.43\noffshore\n16\n0.302\n42328.32\n20800000\n\n\n2\n2\n25.069138\n-97.482483\n8.19\noffshore\n16\n0.375\n52560.00\n20800000\n\n\n3\n3\n25.069443\n-97.463135\n8.19\noffshore\n16\n0.375\n52560.00\n20800000\n\n\n4\n4\n25.069763\n-97.443756\n8.19\noffshore\n16\n0.376\n52700.16\n20800000\n\n\n\n\n\n\n\nSource: Data Preview\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nirradiance\nfarm_type\ncapacity\ncapacity_factor\npower_generation\nestimated_cost\n\n\n\n\n0\n0\n25.896492\n-97.460358\n5.634079\nlarge_community\n5.00\n0.235\n10282.193270\n13300000\n\n\n1\n1\n26.032654\n-97.738098\n5.616413\nsmall_utility\n5.00\n0.234\n10249.953070\n13300000\n\n\n2\n2\n26.059063\n-97.208252\n5.746738\nsmall_community\n0.15\n0.239\n314.633929\n399000\n\n\n3\n3\n26.078449\n-98.073364\n5.742196\nsmall_utility\n5.00\n0.239\n10479.506980\n13300000\n\n\n4\n4\n26.143227\n-98.311340\n5.817187\nsmall_utility\n5.00\n0.242\n10616.365970\n13300000\n\n\n\n\n\n\n\nSource: Data Preview\nThe rough and simplistic nature of these estimates are generally okay as the concern of this project is to determine how well machine learning algorithms predict values, and not the correctness and integrity of the values themselves. Nonetheless, issues still appeared because of this. When training and testing the Random Forest Regressor, a feature importance graph was used to determine which attributes are the most important. This can also give insights into how the variables are used by other models as well. When running on the old wind data, the graph indicated one variable was dominating the dataset.\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nSource: Old Random Forest Model\n\n\n\nFeature Importances\n----------------------------\nlat: 1.7415146179729832e-09\nlong: 4.556724073707158e-09\nwind_speed: 2.5874951940190983e-08\ncapacity_factor: 0.0005676720796435423\ncapacity: 99.99943229574717\n\n\nSource: Old Random Forest Model\nOut of all of the features, capacity was shown to have the highest importance. This makes sense as both calculations are made up of direct proportions that scale based on capacity. This then indicates that the issue is not Random Forest exclusive, as this bias is present in the dataset, and not in how the algorithm interprets the dataset. Changes had to be implemeneted to remove this bias, which included adding more features and using more in-depth calculations to derive the features to be predicted. These are covered in The New Data.\n\n\n\n\nThe initial direction of this project was to look specifically into clustering techniques and cluster analysis. This process consists of grouping similar data into a specified number of clusters. When considering the geospatial data used for this project, clustering would be used to abstract coordinates in the form of longitude and latitude into a single value, a cluster ID. The cluster ID’s would then represent the other features of the data point, allowing for predictions of other features to be made based off of a given cluster ID or coordinate location. The main two steps for performing the clustering is breifly outlined below:\n\nDetermine the number of optimal clusters for the dataset.\n\nTo do this, you just need to cluster the data with varying cluster numbers, score them, and graph the scores by the number of clusters and determine the point at which gains start to diminish.\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nSource: Data Clustering\n\nCluster the data and plot the results\n\nWith the optimal number of clusters (or any desired number), the data can be clustered, with each data point given a cluster ID.\n\n\n\n\n\n\n\n\n\n\nlat\nlong\ncluster_label\n\n\n\n\n0\n23.510410\n-117.147260\n7\n\n\n1\n24.007446\n-93.946777\n6\n\n\n2\n25.069138\n-97.482483\n6\n\n\n3\n25.069443\n-97.463135\n6\n\n\n4\n25.069763\n-97.443756\n6\n\n\n\n\n\n\n\nSource: Data Clustering\nNow that clusters are generated, they can be plotted and visualized.\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nSource: Data Clustering\nThis not only shows the shape and dimensions of the data, but it also gives insights into how many clusters are suitable for the task. The scale of the data makes the number of optimal clusters here a bit ambiguous. Although around 20 seems to be the best, 20 clusters does not seem to represent the nuances of how geospatial data like wind speed and solar raditation changes throughout the country. This seems to indicate that this approach is a bit unreliable in that regard and could introduce more bias into the data and models.\nAdditionally, the clustering could also help increase the performance of the algorithms by lowering the number of input parameters. Simplifying the inputs allows the models to interpret the data easier, leading to better predictions. The issues described in the structure and make-up section further examplified this potential benefit. But for those same reasons, clustering ended up not being an ideal pathway forward. Not only would refactoring the data to remove the bias negate the benefits of clustering in the long run, but clustering also ends up become computationally expensive and tedious to implement and analyze given the nature of the data. For these reasons it was discarded as an aspect of the project."
  },
  {
    "objectID": "data.html#the-new-data",
    "href": "data.html#the-new-data",
    "title": "A Look at the Data",
    "section": "",
    "text": "To overcome the issues outlined above, the datasets were both refactored heavily to remove the bias present. This fixed structural issues, but also allowed for in-depth analysis of mahcine learning models without the time and efficiency overheads that would be present if clustering remained a crucial piece of the project. The process and results of refactoring both of the datasets are outlined below:\n\n\nTo reduce the bias in the wind dataset, new features were added and a new equation was used that incorporated more features than just capacity to ensure a wider range of importance across the board. Two new features were added:\n\nState - the state that the latitude and longitude coordinates are within.\nLevelized Cost of Energy (LCOE) - the average net cost of electricity generation for a generator over its lifetime.\n\nBoth of these features gave way for more inclusive calculations to be used for both energy generation and cost. The updated method to find the energy generation, \\(J\\), for a given wind farm uses the following equation where \\(c\\) is capacity, \\(\\rho\\) is the density of air, \\(A\\) is the swept area of the wind turbine, \\(v\\) is the wind speed, \\(h\\) is the number of hours in a year, and \\(c_f\\) is capacity factor:\n\n\\(J = \\frac{c}{2}\\left( \\frac{1}{2}\\rho Av^3\\right)h c_f\\)\nSome values here are assumed constants: \\(\\rho = 1.225 kg/m^3\\), \\(A = 7854 m^2\\), \\(h = 8760hrs\\)\n\nAlso, this equation was split at certain parts to increase the number of features, to aide the model in being able to track the relationships better. The splits added the features available_wind_power and available_energy.\nCalculating the cost using LCOE is very simple. A technology’s LCOE is measured in dollars per watt-hour of energy generated. The only other piece needed is the lifespan of the technology. For this project, a 20 year lifespan is assumed for all wind turbines. This results in this equation where \\(J\\) is generated energy, \\(l\\) is the LCOE of the technology in the state, and \\(t\\) is the lifespan of the technology:\n\n\\(C = Jlt\\)\n\nThese equations incorporate crucial pieces of geospatial data that were not present in the previous calculation. It also adds other assumed constants, which add complexity to the calculation and obscure the relationships a bit, giving the model more agency to work out the connections. In the end, it results in a dataset that looks like this:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nonshore\n7.46\n31\n2\n0.433\n1.997163\n17495.14630\n7575.398348\n4.696747e+06\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nonshore\n7.45\n31\n10\n0.414\n9.945710\n87124.42376\n36069.511440\n2.236310e+07\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nonshore\n8.18\n31\n2\n0.506\n2.633037\n23065.40088\n11671.092850\n7.236078e+06\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nonshore\n7.17\n31\n16\n0.363\n14.185493\n124264.92160\n45108.166540\n2.796706e+07\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nonshore\n7.06\n31\n16\n0.358\n13.542570\n118632.91080\n42470.582050\n2.633176e+07\n\n\n\n\n\n\n\nSource: Data Preview\nThis dataset can be accessed and downloaded from here.\n\n\n\nSimilar to the wind dataset, bias was reduced by adding more features and using a new equation which incorporated more features. Like the wind data, to make these adjustments, state and lcoe needed to be added as features. On top of these, two other features were dervied as part of the calculation to determine the generated energy. These are as follows:\n\nArray Area, where \\(c\\) is capacity, \\(I\\) is the solar irradiance and \\(c_f\\) is the capacity factor.\n\n\n\\(A = \\frac{24c}{Ic_f}\\)\nWe multiply by 24 because \\(I\\) is in units of \\(kWh/m^2/day\\) and we want it in units of \\(kWh/m^2\\) to ensure we end up with an area as the result.\n\n\nAvailable Solar Resource, where \\(I\\) is the solar irradiance and \\(A\\) is the array area.\n\n\n\\(P = \\frac{IA}{24*1000}\\)\nThe 24 serves the same purpose as before, but the 1000 here changes the units from \\(kWh\\) to \\(MWh\\) to stay consistent all results.\n\nWith these, an equation for generated energy, \\(J\\) can be derived, where \\(P\\) is the available solar resource, \\(h\\) is the number of hours in a year, \\(c_f\\) is the capacity factor, and \\(q\\) is the system losses, which is assumed to be 14%:\n\n\\(J = P h  c_f  q\\)\n\nSame as the wind data, calculating the cost using the LCOE is very simple. A lifespan of 20 years is used again for consistency:\n\n\\(C = Jlt\\)\n\nThe chain of calculations incorporate more of the geospatial data, which exemplifies the purpose of the research project much more than what the old data had, even with clustering. Again, it allows the model to do its best at finding the relationship, compared to just recognizing a direct proportion like before. The resulting solar dataset looks like this:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nirradiance\nlcoe\ncapacity\ncapacity_factor\narray_area\navailable_solar_resource\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nlarge_community\n5.634079\n39\n5.00\n0.235\n90633.862770\n21.276596\n6132.00\n4782960.0\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nsmall_utility\n5.616413\n39\n5.00\n0.234\n91307.484990\n21.367521\n6132.00\n4782960.0\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nsmall_community\n5.746738\n39\n0.15\n0.239\n2621.097459\n0.627615\n183.96\n143488.8\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nsmall_utility\n5.742196\n39\n5.00\n0.239\n87439.036330\n20.920502\n6132.00\n4782960.0\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nsmall_utility\n5.817187\n39\n5.00\n0.242\n85241.850210\n20.661157\n6132.00\n4782960.0\n\n\n\n\n\n\n\nSource: Data Preview\nThis dataset is also available to be accessed and downloaded here."
  },
  {
    "objectID": "rf.html",
    "href": "rf.html",
    "title": "Random Forest Regression on Geospatial Data",
    "section": "",
    "text": "The Random Forest model is an simple, easy to use model that offers good results on a consistent basis for a wide range of applications. One of the areas that it is known to perform quite well in, is geospatial applications. Because of this, there is no doubt it was one of the algorithms chosen for this projects’ analysis. The “regressor” in Random Forest Regressor, stems from the use of strictly numeric data points in the project, which is analyzed most effectively through regressions. Below, the steps to use the Random Forest Regressor model from sklearn on the datasets in breifly outlined. To fill in those blanks, the relative notebooks are linked on the side.\n\n\nThe process for getting to trained model is flushed out in detail below. The other algorithms covered will not be covered in as great as detail for the sake of reducing repetition. Any differences that stand out between the models will be covered on their respective page. Otherwise, the process remains generally the same, if not exactly the same, at a high level.\n\n\nThe first step when performing any kind of machine learning problem or analysis is to read in the data, make alterations to it, and select features for training and testing. For this showcase, we will look at the wind data:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nonshore\n7.46\n31\n2\n0.433\n1.997163\n17495.14630\n7575.398348\n4.696747e+06\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nonshore\n7.45\n31\n10\n0.414\n9.945710\n87124.42376\n36069.511440\n2.236310e+07\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nonshore\n8.18\n31\n2\n0.506\n2.633037\n23065.40088\n11671.092850\n7.236078e+06\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nonshore\n7.17\n31\n16\n0.363\n14.185493\n124264.92160\n45108.166540\n2.796706e+07\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nonshore\n7.06\n31\n16\n0.358\n13.542570\n118632.91080\n42470.582050\n2.633176e+07\n\n\n\n\n\n\n\nSource: Data Preview\nThe dataset as it is, potentially has a lot of bias due to its structure and ordering. Although a human may not be able to see a pattern in the order the way the data as been recorded, a machine learning model is very sensitive to nuances like this. To reduce this being a factor, the dataset must be shuffled. A method from the Python package pandas can do this very easily, that method being sample(). Once it is used, the dataset ends up looking like this:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n104321\n104321\n47.115517\n-106.994720\nMontana\nonshore\n8.08\n31\n16\n0.439\n20.301170\n177838.24560\n78070.98984\n48404013.70\n\n\n44108\n44108\n40.504593\n-87.896454\nIllinois\nonshore\n7.16\n36\n16\n0.374\n14.126223\n123745.70950\n46280.89535\n33322244.65\n\n\n19515\n19515\n36.364742\n-88.897675\nTennessee\nonshore\n6.74\n45\n16\n0.387\n11.783293\n103221.64420\n39946.77632\n35952098.69\n\n\n4940\n4940\n32.670708\n-102.887939\nTexas\nonshore\n7.34\n31\n16\n0.411\n15.218616\n133315.07550\n54792.49602\n33971347.53\n\n\n70776\n70776\n39.942379\n-122.230392\nCalifornia\nonshore\n6.11\n47\n16\n0.338\n8.778304\n76897.94144\n25991.50421\n24432013.95\n\n\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nWith the data shuffled, we can now ensure that the training and testing splits have as little bias as possible, from an ordering perspective. Two different arrays must be made for each split, those being X, the inputs, and y, the outputs, or features to be predicted. For this project, we are interested in determining if a machine learning model is effective when only given locational data, and as little extra information as possible. So for our X variables, we chose lat, long, and capacity. For the y, generated_energy and cost were used, since these are the values we want to predict. The idea is that a hypothetical user should only need to input their location, and how big they want the wind turbine/farm to be, if they want predictions. For the splits, we reserve about 80% of the data for training, and 20% for testing.\nX = df.loc[:, ['lat','long','capacity']]\ny = df.loc[:, ['generated_energy','cost']]\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_train = y[:100000]\ny_test = y[100000:]\n\nNotice that we are using two targets, this is because we are taking advantage of multioutput regression functionality. This saves on time and computational costs by creating one model that can predict two targets with one set of features.\n\nWith shuffled training and testing splits, some changes must be made to the data to ensure it is easier for the model to process. Many machine learning models prefer having smaller numbers, ideally numbers between [1,-1]. From the dataset above, this is clearly not the case. To get close to this ideal, a scaler is used. A scaler will take the entire dataset, and apply a transformation to every feature, making the numbers smaller, and easier for a model to work with, without losing the meaning the original data had. For this project, sklearn’s StandardScaler was used. Note: The scaler is only used on the inputs, not on the outputs. It would not change the results, but it would supply an illusion that the model is performing better.\n\n\n\narray([[ 1.50475958, -0.69192413,  0.51463162],\n       [-0.00765112,  0.91790859,  0.51463162],\n       [-0.95474337,  0.83351358,  0.51463162],\n       ...,\n       [ 0.20099693, -0.87590282, -0.07786604],\n       [-0.09467121, -0.45629192,  0.51463162],\n       [ 1.00200959,  0.73976282,  0.51463162]])\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nNow, the data is ready to be used to train a Random Forest Regressor model.\n\n\n\nThis research is interested in determining if simple machine learning models can effectively predict renewable energy array parameters. To exaggerate this point, the models were used in the most out-of-the-box way possible, keeping things very simple. To train the model, the training data is passed into the Random Forest Regressor:\nreg = RandomForestRegressor()\nreg.fit(X_train, y_train)\nThis ends up creating a large amount of individual decision trees, making up a forest. Each tree uses the input features as decision making points, splitting nodes in the tree based on how well a feature is contributing towards making better predictions. Because of the scale of the data, the trees are very large, but the general structure of the first few nodes looks like this:\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nWith a trained model, predictions can be made using the predict() method. Below are some predicted values compared to the true values.\n\n\n\nPredictions\n----------------------\npredicted energy: 39496.22  actual energy: 40307.25 predicted cost: 23697732.20 actual cost: 24184348.66\npredicted energy: 102419.64 actual energy: 101556.00    predicted cost: 61451784.90 actual cost: 60933600.27\npredicted energy: 49848.14  actual energy: 128958.46    predicted cost: 32899772.22 actual cost: 85112586.37\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nThe values are fairly close, especially when considering how little information the model is using to make the predictions. This indicates that there is some potential that general location-based information is enough to estimate the energy generation and cost of a wind turbine/farm.\n\n\n\n\nThere are many ways to evaluate the performace of a model. Some of these are model dependent, like feature importance for a Random Forest. Others can be done to any model, like reporting metrics, or performing k-fold cross validation. Outlined below, are the strategies used to evaluate the performance of the Random Forest Regressor used in this project.\n\n\nA helpful way to assess a machine learning model is to plot the model against the data being used. This allows for one to see the how the predictive model is fitting to the data. Below are the graphs for each feature, for each target:\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nThe curves are not as smooth as one might hope, but they seem to represent the data in a fair way, although it is hard to fully capture due to how many data points there really are. While being able to visualize the models in the context of the data, more information is needed to truly be able to evaluate how well the model is performing. For that, we use metrics.\n\nIt is important to note that these are the predictive models for each feature, and not the overall curve for the entire model.\n\n\n\n\nTo evaluate the machine learning models, three metrics, available through sklearn, were chosen.\n\nR2-score (R2) - a goodness-of-fit metric that indicates how well a plotted curve represents the data.\nRoot Mean Squared Error (RMSE) - an error metric that explicitly states the average distance between a predicted value and its true counterpart.\nMean Absolute Percentage Error (MAPE) - a goodness-of-fit metric that indicates the average percent error a prediction has relative to the true values.\n\nR2 and MAPE were chosen because they are very scale-independent, meaning the difference in size of the data’s features and outliers have less of an effect on the score. The data has many outliers, and the features values range drastically in size, sometimes by many orders of magnitude. Scale-independent metrics help to isolate these edge cases and provide scoring that is relative to the majority of the dataset.\nRMSE was chosen because, unlike R2 and MAPE, it is very scale-dependent. Utilizing both types of metrics ensures that the full picture is uncovered, giving a better overall summary of performance. Even though the scale of the data and the outliers may abstract the meaning of the score somewhat, the scale can be used to help explain why the metric appears the way it does. In the end, it still provides valuable insights into the performance that could indicate pathways for improvement in the future.\nA single report of metrics for an isolated training and testing session looks like this:\n\n\n\nMetric  Score\n-----------------------\nr2  [0.9223821  0.89367722]\nrmse    [   8843.51723011 6421746.41274378]\nmape    [0.12902421 0.1288721 ]\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\n\nThe metrics are separated for each target variable, the values on the left are for generated_energy predictions and the values on the right are for cost predictions.\n\nR2 values around .90 indicate desireable results. This means that 90% of the variance in the dataset can be explained by the model, meaning it is fitting the data well. The MAPE values rienforce this idea too. While MAPE values below 10% would be the most ideal, a MAPE of 13% indicates predictions are on average 13% off of what they should be. This is generally considered “good”.\nThe RMSE is where things get tricky. Individually, they are not too awful. Although they look large, they are inflated due to the scale of the data. When considering that the majority of the data for generated energy is in the tens of thousands, and for cost it is in the tens of millions, the ratio of the RMSE to this “median” value can be roughly approximated to around .30, or 30% for both targets. For a metric that is not best suited for a dataset of this type, 30% also can indicate promising results. Even though it is more than twice the MAPE, this is to be expected because of the scale issues mentioned prior. In the end, it highlights the issue posed by the dataset, but also can somewhat reinforce the idea that the model is performing well, if examined deep enough.\n\n\n\nIn a Random Forest, the input features end up being “ranked” by importance. This means that some features are used to determine how the tree splits more than others. A higher importance indicates that a feature is being used more often, while a lower importance indicates the opposite. When assessing a Random Forest, it is nice to see an even split of importance across all features. This would indicate that all of the features are being used equally to get to predictions. This not only ensures the model is functioning properly, but it also ensures that the dataset is not skewed towards a single feature.\nA plot of the feature importances, and their respective values are shown below:\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\n\n\n\nImportances\n----------------------\ncapacity: 20.230960634451716\nlat: 35.095811124953094\nlong: 44.67322824059519\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nWhile it is not an even split, each feature has an importance that is significant when regarding the creation of the model. This indicates that the model is functioning fairly well with how it is using the data, and it even goes so far as to reinforce that the dataset is construced in a meaningful and unbiased way. All of this points to the model being effective at making trustworthy predictions.\n\n\n\nThe last piece of assessment comes from k-fold cross validation. K-fold cross validation consists of splitting up the dataset into k folds, and using those folds for training and testing. Each fold produces a trained model that then has metrics taken from it, in this case it is the same metrics listed above. The end goal being to obtain averages for all of the desired metrics over the entire dataset. Even though we shuffled the dataset, some of the data is not being used for training. K-fold cross validation ensures that all of the data is being used for both training and testing at some point, resulting in more accurate reporting of metrics.\nFor this project, 10 folds were used. The results of running k-fold cross validation on the wind dataset is shown below.\n\n\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.9109908237527048\nRMSE Average: 3181739.2057383326\nMAPE Average: 0.13390596472645255\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n35.631838\n0.412091\n0.908636\n3.244508e+06\n0.126463\n\n\n1\n35.085633\n0.407737\n0.909902\n3.194235e+06\n0.134273\n\n\n2\n34.784758\n0.409132\n0.912724\n3.128678e+06\n0.129694\n\n\n3\n34.819360\n0.401673\n0.908216\n3.253428e+06\n0.126229\n\n\n4\n34.879103\n0.404808\n0.907344\n3.241619e+06\n0.137367\n\n\n5\n34.481580\n0.417363\n0.915569\n3.113150e+06\n0.137573\n\n\n6\n35.357341\n0.434344\n0.908085\n3.232867e+06\n0.148030\n\n\n7\n37.669082\n0.456003\n0.915380\n3.138264e+06\n0.137533\n\n\n8\n37.000147\n0.439912\n0.913334\n3.138594e+06\n0.128061\n\n\n9\n34.930129\n0.409387\n0.910719\n3.132047e+06\n0.133837\n\n\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nK-fold cross validation through sklearn does not support multioutput. Because of this, the metrics for both targets are averaged together, resulting in one metric that defines the model’s overall performance. This is okay for R2 and MAPE, as they are similar enough and scale-independent, so little information is lost or abstracted. RMSE, however, becomes slightly less meaningfull when averaged due to the difference in scale between generated_energy and cost. Averaging the separate RMSE’s from a single report results in an average RMSE of 3,137,778.11. Comparing this to the average RMSE from cross validation, they are fairly similar. It can then be assumed that the individual RMSE’s would be around the same, indicating that the averages for then entire dataset are still within reasonable bounds. A similar conclusion can be drawn regarding R2 and MAPE. These values are within the same bounds of acceptance as they were in the metrics section, indicating good results.\n\n\n\n\nThe solar data is not, and will, not be covered in detail like the wind data was above. It was concluded that the solar data is insufficient for the project’s goals. The metrics that come from a model trained on solar data indicate more than just poor results. It is omitted to reduce confusion and bring to light the more impactful results of this experiment.\n\n\n\nMetric  Score\n-----------------------\nr2  [1.         0.99947801]\nrmse    [1.89379452e-12 1.36960035e+07]\nmape    [9.69285180e-16 4.32843758e-03]\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nNumbers that look like this are intrinsic of bad models or bad data. In this case, it is bad data. The issue stems from what data was available publically and feasible to work with. Many gaps needed to be filled in, and not enough data was available to fill these gaps in in a way that did not comprimise the dataset in the end. The notebook is still available to view for purposes of experiment replication and validation."
  },
  {
    "objectID": "rf.html#getting-a-trained-model",
    "href": "rf.html#getting-a-trained-model",
    "title": "Random Forest Regression on Geospatial Data",
    "section": "",
    "text": "The process for getting to trained model is flushed out in detail below. The other algorithms covered will not be covered in as great as detail for the sake of reducing repetition. Any differences that stand out between the models will be covered on their respective page. Otherwise, the process remains generally the same, if not exactly the same, at a high level.\n\n\nThe first step when performing any kind of machine learning problem or analysis is to read in the data, make alterations to it, and select features for training and testing. For this showcase, we will look at the wind data:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n0\n0\n25.896492\n-97.460358\nTexas\nonshore\n7.46\n31\n2\n0.433\n1.997163\n17495.14630\n7575.398348\n4.696747e+06\n\n\n1\n1\n26.032654\n-97.738098\nTexas\nonshore\n7.45\n31\n10\n0.414\n9.945710\n87124.42376\n36069.511440\n2.236310e+07\n\n\n2\n2\n26.059063\n-97.208252\nTexas\nonshore\n8.18\n31\n2\n0.506\n2.633037\n23065.40088\n11671.092850\n7.236078e+06\n\n\n3\n3\n26.078449\n-98.073364\nTexas\nonshore\n7.17\n31\n16\n0.363\n14.185493\n124264.92160\n45108.166540\n2.796706e+07\n\n\n4\n4\n26.143227\n-98.311340\nTexas\nonshore\n7.06\n31\n16\n0.358\n13.542570\n118632.91080\n42470.582050\n2.633176e+07\n\n\n\n\n\n\n\nSource: Data Preview\nThe dataset as it is, potentially has a lot of bias due to its structure and ordering. Although a human may not be able to see a pattern in the order the way the data as been recorded, a machine learning model is very sensitive to nuances like this. To reduce this being a factor, the dataset must be shuffled. A method from the Python package pandas can do this very easily, that method being sample(). Once it is used, the dataset ends up looking like this:\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlong\nstate\nfarm_type\nwind_speed\nlcoe\ncapacity\ncapacity_factor\navailable_wind_power\navailable_energy\ngenerated_energy\ncost\n\n\n\n\n104321\n104321\n47.115517\n-106.994720\nMontana\nonshore\n8.08\n31\n16\n0.439\n20.301170\n177838.24560\n78070.98984\n48404013.70\n\n\n44108\n44108\n40.504593\n-87.896454\nIllinois\nonshore\n7.16\n36\n16\n0.374\n14.126223\n123745.70950\n46280.89535\n33322244.65\n\n\n19515\n19515\n36.364742\n-88.897675\nTennessee\nonshore\n6.74\n45\n16\n0.387\n11.783293\n103221.64420\n39946.77632\n35952098.69\n\n\n4940\n4940\n32.670708\n-102.887939\nTexas\nonshore\n7.34\n31\n16\n0.411\n15.218616\n133315.07550\n54792.49602\n33971347.53\n\n\n70776\n70776\n39.942379\n-122.230392\nCalifornia\nonshore\n6.11\n47\n16\n0.338\n8.778304\n76897.94144\n25991.50421\n24432013.95\n\n\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nWith the data shuffled, we can now ensure that the training and testing splits have as little bias as possible, from an ordering perspective. Two different arrays must be made for each split, those being X, the inputs, and y, the outputs, or features to be predicted. For this project, we are interested in determining if a machine learning model is effective when only given locational data, and as little extra information as possible. So for our X variables, we chose lat, long, and capacity. For the y, generated_energy and cost were used, since these are the values we want to predict. The idea is that a hypothetical user should only need to input their location, and how big they want the wind turbine/farm to be, if they want predictions. For the splits, we reserve about 80% of the data for training, and 20% for testing.\nX = df.loc[:, ['lat','long','capacity']]\ny = df.loc[:, ['generated_energy','cost']]\n\nX_train = X[:100000]\nX_test = X[100000:]\ny_train = y[:100000]\ny_test = y[100000:]\n\nNotice that we are using two targets, this is because we are taking advantage of multioutput regression functionality. This saves on time and computational costs by creating one model that can predict two targets with one set of features.\n\nWith shuffled training and testing splits, some changes must be made to the data to ensure it is easier for the model to process. Many machine learning models prefer having smaller numbers, ideally numbers between [1,-1]. From the dataset above, this is clearly not the case. To get close to this ideal, a scaler is used. A scaler will take the entire dataset, and apply a transformation to every feature, making the numbers smaller, and easier for a model to work with, without losing the meaning the original data had. For this project, sklearn’s StandardScaler was used. Note: The scaler is only used on the inputs, not on the outputs. It would not change the results, but it would supply an illusion that the model is performing better.\n\n\n\narray([[ 1.50475958, -0.69192413,  0.51463162],\n       [-0.00765112,  0.91790859,  0.51463162],\n       [-0.95474337,  0.83351358,  0.51463162],\n       ...,\n       [ 0.20099693, -0.87590282, -0.07786604],\n       [-0.09467121, -0.45629192,  0.51463162],\n       [ 1.00200959,  0.73976282,  0.51463162]])\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nNow, the data is ready to be used to train a Random Forest Regressor model.\n\n\n\nThis research is interested in determining if simple machine learning models can effectively predict renewable energy array parameters. To exaggerate this point, the models were used in the most out-of-the-box way possible, keeping things very simple. To train the model, the training data is passed into the Random Forest Regressor:\nreg = RandomForestRegressor()\nreg.fit(X_train, y_train)\nThis ends up creating a large amount of individual decision trees, making up a forest. Each tree uses the input features as decision making points, splitting nodes in the tree based on how well a feature is contributing towards making better predictions. Because of the scale of the data, the trees are very large, but the general structure of the first few nodes looks like this:\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nWith a trained model, predictions can be made using the predict() method. Below are some predicted values compared to the true values.\n\n\n\nPredictions\n----------------------\npredicted energy: 39496.22  actual energy: 40307.25 predicted cost: 23697732.20 actual cost: 24184348.66\npredicted energy: 102419.64 actual energy: 101556.00    predicted cost: 61451784.90 actual cost: 60933600.27\npredicted energy: 49848.14  actual energy: 128958.46    predicted cost: 32899772.22 actual cost: 85112586.37\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nThe values are fairly close, especially when considering how little information the model is using to make the predictions. This indicates that there is some potential that general location-based information is enough to estimate the energy generation and cost of a wind turbine/farm."
  },
  {
    "objectID": "rf.html#analyzing-and-assesing-the-random-forest",
    "href": "rf.html#analyzing-and-assesing-the-random-forest",
    "title": "Random Forest Regression on Geospatial Data",
    "section": "",
    "text": "There are many ways to evaluate the performace of a model. Some of these are model dependent, like feature importance for a Random Forest. Others can be done to any model, like reporting metrics, or performing k-fold cross validation. Outlined below, are the strategies used to evaluate the performance of the Random Forest Regressor used in this project.\n\n\nA helpful way to assess a machine learning model is to plot the model against the data being used. This allows for one to see the how the predictive model is fitting to the data. Below are the graphs for each feature, for each target:\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nThe curves are not as smooth as one might hope, but they seem to represent the data in a fair way, although it is hard to fully capture due to how many data points there really are. While being able to visualize the models in the context of the data, more information is needed to truly be able to evaluate how well the model is performing. For that, we use metrics.\n\nIt is important to note that these are the predictive models for each feature, and not the overall curve for the entire model.\n\n\n\n\nTo evaluate the machine learning models, three metrics, available through sklearn, were chosen.\n\nR2-score (R2) - a goodness-of-fit metric that indicates how well a plotted curve represents the data.\nRoot Mean Squared Error (RMSE) - an error metric that explicitly states the average distance between a predicted value and its true counterpart.\nMean Absolute Percentage Error (MAPE) - a goodness-of-fit metric that indicates the average percent error a prediction has relative to the true values.\n\nR2 and MAPE were chosen because they are very scale-independent, meaning the difference in size of the data’s features and outliers have less of an effect on the score. The data has many outliers, and the features values range drastically in size, sometimes by many orders of magnitude. Scale-independent metrics help to isolate these edge cases and provide scoring that is relative to the majority of the dataset.\nRMSE was chosen because, unlike R2 and MAPE, it is very scale-dependent. Utilizing both types of metrics ensures that the full picture is uncovered, giving a better overall summary of performance. Even though the scale of the data and the outliers may abstract the meaning of the score somewhat, the scale can be used to help explain why the metric appears the way it does. In the end, it still provides valuable insights into the performance that could indicate pathways for improvement in the future.\nA single report of metrics for an isolated training and testing session looks like this:\n\n\n\nMetric  Score\n-----------------------\nr2  [0.9223821  0.89367722]\nrmse    [   8843.51723011 6421746.41274378]\nmape    [0.12902421 0.1288721 ]\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\n\nThe metrics are separated for each target variable, the values on the left are for generated_energy predictions and the values on the right are for cost predictions.\n\nR2 values around .90 indicate desireable results. This means that 90% of the variance in the dataset can be explained by the model, meaning it is fitting the data well. The MAPE values rienforce this idea too. While MAPE values below 10% would be the most ideal, a MAPE of 13% indicates predictions are on average 13% off of what they should be. This is generally considered “good”.\nThe RMSE is where things get tricky. Individually, they are not too awful. Although they look large, they are inflated due to the scale of the data. When considering that the majority of the data for generated energy is in the tens of thousands, and for cost it is in the tens of millions, the ratio of the RMSE to this “median” value can be roughly approximated to around .30, or 30% for both targets. For a metric that is not best suited for a dataset of this type, 30% also can indicate promising results. Even though it is more than twice the MAPE, this is to be expected because of the scale issues mentioned prior. In the end, it highlights the issue posed by the dataset, but also can somewhat reinforce the idea that the model is performing well, if examined deep enough.\n\n\n\nIn a Random Forest, the input features end up being “ranked” by importance. This means that some features are used to determine how the tree splits more than others. A higher importance indicates that a feature is being used more often, while a lower importance indicates the opposite. When assessing a Random Forest, it is nice to see an even split of importance across all features. This would indicate that all of the features are being used equally to get to predictions. This not only ensures the model is functioning properly, but it also ensures that the dataset is not skewed towards a single feature.\nA plot of the feature importances, and their respective values are shown below:\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\n\n\n\nImportances\n----------------------\ncapacity: 20.230960634451716\nlat: 35.095811124953094\nlong: 44.67322824059519\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nWhile it is not an even split, each feature has an importance that is significant when regarding the creation of the model. This indicates that the model is functioning fairly well with how it is using the data, and it even goes so far as to reinforce that the dataset is construced in a meaningful and unbiased way. All of this points to the model being effective at making trustworthy predictions.\n\n\n\nThe last piece of assessment comes from k-fold cross validation. K-fold cross validation consists of splitting up the dataset into k folds, and using those folds for training and testing. Each fold produces a trained model that then has metrics taken from it, in this case it is the same metrics listed above. The end goal being to obtain averages for all of the desired metrics over the entire dataset. Even though we shuffled the dataset, some of the data is not being used for training. K-fold cross validation ensures that all of the data is being used for both training and testing at some point, resulting in more accurate reporting of metrics.\nFor this project, 10 folds were used. The results of running k-fold cross validation on the wind dataset is shown below.\n\n\n\n10-Fold Cross Validation Scores\n----------------------------------------------------\nR2 Average: 0.9109908237527048\nRMSE Average: 3181739.2057383326\nMAPE Average: 0.13390596472645255\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_r2\ntest_rmse\ntest_mape\n\n\n\n\n0\n35.631838\n0.412091\n0.908636\n3.244508e+06\n0.126463\n\n\n1\n35.085633\n0.407737\n0.909902\n3.194235e+06\n0.134273\n\n\n2\n34.784758\n0.409132\n0.912724\n3.128678e+06\n0.129694\n\n\n3\n34.819360\n0.401673\n0.908216\n3.253428e+06\n0.126229\n\n\n4\n34.879103\n0.404808\n0.907344\n3.241619e+06\n0.137367\n\n\n5\n34.481580\n0.417363\n0.915569\n3.113150e+06\n0.137573\n\n\n6\n35.357341\n0.434344\n0.908085\n3.232867e+06\n0.148030\n\n\n7\n37.669082\n0.456003\n0.915380\n3.138264e+06\n0.137533\n\n\n8\n37.000147\n0.439912\n0.913334\n3.138594e+06\n0.128061\n\n\n9\n34.930129\n0.409387\n0.910719\n3.132047e+06\n0.133837\n\n\n\n\n\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nK-fold cross validation through sklearn does not support multioutput. Because of this, the metrics for both targets are averaged together, resulting in one metric that defines the model’s overall performance. This is okay for R2 and MAPE, as they are similar enough and scale-independent, so little information is lost or abstracted. RMSE, however, becomes slightly less meaningfull when averaged due to the difference in scale between generated_energy and cost. Averaging the separate RMSE’s from a single report results in an average RMSE of 3,137,778.11. Comparing this to the average RMSE from cross validation, they are fairly similar. It can then be assumed that the individual RMSE’s would be around the same, indicating that the averages for then entire dataset are still within reasonable bounds. A similar conclusion can be drawn regarding R2 and MAPE. These values are within the same bounds of acceptance as they were in the metrics section, indicating good results."
  },
  {
    "objectID": "rf.html#a-note-about-the-solar-data",
    "href": "rf.html#a-note-about-the-solar-data",
    "title": "Random Forest Regression on Geospatial Data",
    "section": "",
    "text": "The solar data is not, and will, not be covered in detail like the wind data was above. It was concluded that the solar data is insufficient for the project’s goals. The metrics that come from a model trained on solar data indicate more than just poor results. It is omitted to reduce confusion and bring to light the more impactful results of this experiment.\n\n\n\nMetric  Score\n-----------------------\nr2  [1.         0.99947801]\nrmse    [1.89379452e-12 1.36960035e+07]\nmape    [9.69285180e-16 4.32843758e-03]\n\n\nSource: Random Forest Regression Process and Analysis for Wind Data\nNumbers that look like this are intrinsic of bad models or bad data. In this case, it is bad data. The issue stems from what data was available publically and feasible to work with. Many gaps needed to be filled in, and not enough data was available to fill these gaps in in a way that did not comprimise the dataset in the end. The notebook is still available to view for purposes of experiment replication and validation."
  }
]